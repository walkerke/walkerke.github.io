[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "WALKER DATA",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nExploratory spatial data analysis with Python\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GIS, demographics, and data science consulting",
    "section": "",
    "text": "Welcome to Walker Data, the data science consulting website of Kyle Walker. Please reach out to kyle@walker-data.com for consulting assistance with the following topics:\n\nDemographic analysis / Census data\nBusiness and location intelligence\nGeographic Information Systems (GIS) and spatial data science\nTraining in the R and Python programming languages\n\n\n\n\n\n\n\n  \n    \n      \n      Expert demographics, Census, and data science advisory\n    \n  \n  \n    \n      \n      Sophisticated location intelligence solutions for your business\n    \n  \n  \n    \n      \n      R and GIS training from the developer of the tidycensus R package\n    \n  \n  \n    \n      \n      Read the book Analyzing US Census Data: Methods, Maps, and Models in R\n    \n  \n\n\n\n\nTo receive on-going updates from Walker Data, consider signing up for the Walker Data mailing list:"
  },
  {
    "objectID": "posts/esda-with-python/index.html",
    "href": "posts/esda-with-python/index.html",
    "title": "Exploratory spatial data analysis with Python",
    "section": "",
    "text": "In early 2023, the print copy of my book Analyzing US Census Data: Methods, Maps, and Models in R will be available for purchase. The response to the free online version of the book has been fantastic thus far. One question I commonly get asked, however, is “will you re-produce this for Python? I’d love to use this work but I don’t use R.”\nI don’t have plans to replicate all of my R work in Python, but I did get the itch in the second half of 2022 to learn Python package development. The result is pygris, which is a port of the R tigris package but with some additional features.\nTo celebrate the publication of my book as well as the pygris package, I’m launching a blog series to illustrate how to reproduce some of my favorite examples from my book in Python. Each example will feature pygris. Follow along - I hope you find it useful!"
  },
  {
    "objectID": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "href": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "title": "Exploratory spatial data analysis with Python",
    "section": "Mapping Local Moran’s I (LISA) clusters in Python",
    "text": "Mapping Local Moran’s I (LISA) clusters in Python\nCorresponding section in Analyzing US Census Data: Identifying clusters and spatial outliers with local indicators of spatial association (LISA)\nI came into graduate school intending to be a qualitative researcher, but I really got excited about spatial data analysis (and changed my career trajectory) when I learned GeoDa, a GUI tool for exploratory spatial data analysis. The method in GeoDa that resonated with me the most was the local form of the Moran’s I, an example of a LISA (local indicators of spatial association) statistic.\nLISAs are exploratory tools that help you make sense of spatial patterns in a dataset. They help surface preliminary answers to these questions:\n\nWhere are there concentrations of high attribute values in my spatial dataset?\nConversely, where can I find concentrations of low attribute values in my data?\nFinally, are there any unexpected values in my dataset, given the characteristics of their neighbors? These “spatial outliers” can be above-average values surrounded by low values, or below-average values surrounded by high values.\n\nThis post will walk you through how to create an interactive LISA map of median age by Census tract from the 2017-2021 American Community Survey, similar to the example in Section 7.7.3 of my book. That section of my book covers more technical details about LISA if you are interested in reading further.\n\nGetting the data with pygris\nTo get started, we’ll use pygris to get the data required for our analysis. The core functionality in pygris is a suite of functions to return US Census Bureau TIGER/Line shapefiles as GeoPandas GeoDataFrames.\nLet’s import the tracts() function to demonstrate how this works for the Minneapolis-St. Paul, Minnesota area. We’ll define a list of the seven core Twin Cities counties, and request Census tract boundaries for those counties with tracts(). pygris functions translate state names/abbreviations and county names internally to FIPS codes, so there is no need to look them up.\nThe argument year = 2021 gives back the 2021 version of the Census tract boundaries, which will be important as we’ll be matching to corresponding 2021 ACS data. Finally, the argument cache = True stores the downloaded shapefile in a local cache, which means that I won’t need to download it again from the Census website in future projects.\n\nfrom pygris import tracts\n\ntc_counties = [\"Hennepin\", \"Ramsey\", \"Scott\", \n               \"Carver\", \"Dakota\", \"Washington\", \"Anoka\"]\n\ntc_tracts = tracts(state = \"MN\", county = tc_counties, \n                   year = 2021, cache = True)\n\ntc_tracts.plot()\n\nUsing FIPS code '27' for input 'MN'\n\n\nUsing FIPS code '053' for input 'Hennepin'\nUsing FIPS code '123' for input 'Ramsey'\nUsing FIPS code '139' for input 'Scott'\nUsing FIPS code '019' for input 'Carver'\nUsing FIPS code '037' for input 'Dakota'\nUsing FIPS code '163' for input 'Washington'\nUsing FIPS code '003' for input 'Anoka'\n\n\n<AxesSubplot:>\n\n\n\n\n\nWe’ll next need to grab data on median age and merge to the Census tract shapes. I don’t have plans to implement my R package tidycensus in Python; tidycensus is designed specifically for use within R’s tidyverse and Pythonic interfaces to the Census API like cenpy already exist. However, I’ve always admired Hannah Recht’s work on the R censusapi package, which can connect to all Census Bureau API endpoints. pygris includes a get_census() function inspired by censusapi that developers can use to build interfaces to the data they need.\nLet’s use get_census() to get data on median age at the Census tract level for Minnesota, then merge to our Census tracts for additional analysis.\n\nfrom pygris import validate_state\nfrom pygris.data import get_census\n\nmn_median_age = get_census(dataset = \"acs/acs5\",\n                           variables = \"B01002_001E\",\n                           year = 2021,\n                           params = {\n                             \"for\": \"tract:*\",\n                             \"in\": f\"state:{validate_state('MN')}\"},\n                           guess_dtypes = True,\n                           return_geoid = True\n)\n\nmn_median_age.head()\n\nUsing FIPS code '27' for input 'MN'\n\n\n\n\n\n\n  \n    \n      \n      B01002_001E\n      GEOID\n    \n  \n  \n    \n      1\n      57.8\n      27001770100\n    \n    \n      2\n      51.5\n      27001770200\n    \n    \n      3\n      46.8\n      27001770300\n    \n    \n      4\n      57.7\n      27001770401\n    \n    \n      5\n      49.1\n      27001770402\n    \n  \n\n\n\n\nYou can understand the arguments to get_census() as follows:\n\ndataset is the dataset name on the Census API you are connecting to. Datasets can be found at https://api.census.gov/data.html in the “Dataset Name” column.\nvariables is a string (or list of strings) representing the variable IDs you want for a given dataset. For the 2021 5-year ACS, those variable IDs are found at https://api.census.gov/data/2021/acs/acs5/variables.html.\n\nyear is the year of your data (or end-year for a 5-year ACS sample); the Census API will refer to this as the “vintage” of the data.\nparams is a dict of query parameters to send to the API. Each endpoint will have its own parameters, so you’ll need to spend a little time with the Census API documentation to learn what you can use. In our case, we are requesting data for Census tracts in Minnesota. The built-in validate_state() function can be used here to convert 'MN' to an appropriate FIPS code.\n\nguess_dtypes and return_geoid are convenience parameters that you’ll want to use judiciously. guess_dtypes tries to guess which columns to convert to numeric, and return_geoid tries to find columns to concatenate into a GEOID column that can be used for merging to Census shapes. These arguments won’t be appropriate for every API endpoint.\n\nWith our data in hand, we can do an inner merge and map the result:\n\nimport matplotlib.pyplot as plt\n\ntc_tract_age = tc_tracts.merge(mn_median_age, how = \"inner\", on = \"GEOID\")\n\ntc_tract_age.plot(column = \"B01002_001E\", legend = True)\n\nplt.title(\"Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS\")\n\nText(0.5, 1.0, 'Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS')\n\n\n\n\n\n\n\nAnalyzing spatial clustering with PySAL\nThe PySAL family of Python packages is central to the work of anyone who needs to analyze spatial data in Python. The esda package makes the calculation of the local Moran’s I statistic remarkably smooth. We will generate a Queen’s case spatial weights object (see my book for more technical details) to represent relationships between Census tracts and their neighbors, then call the Moran_Local() function to calculate the LISA statistics.\n\nfrom libpysal import weights\nimport esda\n\ntc_clean = tc_tract_age.copy().dropna().filter(['GEOID', 'B01002_001E', 'geometry'])\n\nw = weights.contiguity.Queen.from_dataframe(tc_clean)\n\nm = esda.Moran_Local(tc_clean['B01002_001E'], w, seed = 1983)\n\ntype(m)\n\nesda.moran.Moran_Local\n\n\nWe get an object of type Moran_Local which stores our analytical result. The documentation provides a comprehensive overview of the object’s attributes. We’ll grab two here and add them back to our Census tract dataset as new columns: q, which is the LISA quadrant (denoting the cluster type); and p_sim, which gives us a pseudo-p-value based on conditional permutation (see here for further discussion of this concept).\nUsing this information, we can identify “significant” and “non-significant” clusters and generate some more informative labels.\n\nimport numpy as np\n\n# We can extract the LISA quadrant along with the p-value from the lisa object\ntc_clean['quadrant'] = m.q\ntc_clean['p_sim'] = m.p_sim\n# Convert all non-significant quadrants to zero\ntc_clean['quadrant'] = np.where(tc_clean['p_sim'] > 0.05, 0, tc_clean['quadrant'])\n\n# Get more informative descriptions\ntc_clean['quadrant'] = tc_clean['quadrant'].replace(\n  to_replace = {\n    0: \"Not significant\",\n    1: \"High-high\",\n    2: \"Low-high\",\n    3: \"Low-low\",\n    4: \"High-low\"\n  }\n)\n\ntc_clean.head()\n\n\n\n\n\n  \n    \n      \n      GEOID\n      B01002_001E\n      geometry\n      quadrant\n      p_sim\n    \n  \n  \n    \n      0\n      27053103700\n      29.3\n      POLYGON ((-93.25825 44.98358, -93.25790 44.983...\n      Not significant\n      0.070\n    \n    \n      1\n      27053104100\n      28.2\n      POLYGON ((-93.31847 44.98174, -93.31847 44.983...\n      Low-low\n      0.043\n    \n    \n      2\n      27053104400\n      32.4\n      POLYGON ((-93.28158 44.97790, -93.28153 44.978...\n      Not significant\n      0.228\n    \n    \n      3\n      27053105100\n      44.6\n      POLYGON ((-93.32873 44.96012, -93.32873 44.960...\n      Not significant\n      0.127\n    \n    \n      4\n      27053105400\n      33.2\n      POLYGON ((-93.26972 44.96807, -93.26926 44.969...\n      Not significant\n      0.310\n    \n  \n\n\n\n\n\n\nBuilding an interactive LISA map\nWe now have all the information necessary to map LISA clusters. I’m going to show a workflow that differs slightly from typical LISA maps like the one illustrated in my book. One disadvantage of static LISA maps is that they assume an analyst has familiarity with the region under study. Without this familiarity, it can be difficult to determine exactly which locations are represented by different cluster types.\nEnter the .explore() GeoDataFrame method in GeoPandas. .explore() is an interface to Leaflet.js through Folium. Simply calling .explore() on a GeoDataFrame gets you started interactively exploring your spatial data; however, the method itself is a fairly full-featured interactive mapping engine.\nWith a little customization, we can build out an informative interactive map showing our LISA analysis of median age by Census tract in the Twin Cities. Here’s how we do it:\n\nWe choose \"quadrant\" as the column to visualize, and pass a list of colors to cmap to align with the typical color scheme used for LISA mapping (with some small modifications to improve visibility).\nlegend = True adds an informative legend, and a muted grey basemap is selected with tiles.\nThe various _kwds parameters are quite powerful, as this is how you will do more fine-grained customization of your map. We’ll reduce the line weight of our polygons to 0.5, and importantly do some customization of the popup to change the column names to informative aliases. Click on a Census tract to see what you get!\n\n\n# Build a LISA cluster map \ntc_clean.explore(column = \"quadrant\", \n                 cmap = [\"red\", \"hotpink\", \"deepskyblue\", \"blue\", \"lightgrey\"], \n                 legend = True, \n                 tiles = \"CartoDB positron\", \n                 style_kwds = {\"weight\": 0.5}, \n                 legend_kwds = { \"caption\": \"LISA quadrant\"}, \n                 tooltip = False, \n                 popup = True,\n                 popup_kwds = {\n                    \"aliases\": [\"GEOID\", \"Median age\", \"LISA quadrant\", \"Pseudo p-value\"]\n                 })\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nOur analytical result shows that younger areas tend to be found nearer to the Minneapolis / St. Paul urban cores, and older areas cluster in the western, southern, and northeastern suburbs. Spatial outliers are scattered throughout the region, and the map’s interactivity allows us to zoom in and click to understand these outliers in greater detail.\nTry out this workflow for yourself, and follow along here for more of my favorite examples from Analyzing US Census Data translated to Python over the next few months."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Kyle Walker, a spatial data science researcher, educator, and consultant. Here is some information about my current work:\n\nI am the author of the book Analyzing US Census Data: Methods, Maps, and Models in R, which is free to read online and is forthcoming with CRC Press in 2022.\n\nI’m an R developer actively working on the following packages:\n\ntidycensus, which helps R users get demographic & spatial data from the US Census Bureau ready-to-go for use in their analyses;\ntigris, which downloads US Census Bureau spatial data and loads it directly into R as simple features objects;\nmapboxapi, an R interface to Mapbox web services. Use the package to optimize routes, draw isochrones, read and write vector tiles, use custom Mapbox maps in Leaflet projects, and more;\ncrsuggest, which gives R users projected coordinate system suggestions for their spatial datasets.\n\n\nI’m an academic researching data science and visualization tools for spatial demography. I’m currently teaching courses in exploratory data analysis with Python and introductory Urban Studies.\n\nI am Director of Research at a boutique data science and strategy firm, where we use cutting-edge geospatial & machine learning methods to improve companies’ business outcomes.\n\nI also consult through my personal firm, Walker Data, where I work with individual clients and organizations to integrate tools like tidycensus into their workflows and to learn R and spatial data analysis.\n\nIf you are interested in working with me, send me a note at kyle@walker-data.com and let’s discuss your idea!"
  }
]