[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "WALKER DATA",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMapping jobs and commutes with 2020 LODES data and deck.gl\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nUsing your favorite Python packages in ArcGIS Pro\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nDistance and proximity analysis in Python\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\nnavigation\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing labor markets in Python with LODES data\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nExploratory spatial data analysis with Python\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nKyle Walker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GIS, demographics, and data science consulting",
    "section": "",
    "text": "Welcome to Walker Data, the data science consulting website of Kyle Walker. Please reach out to kyle@walker-data.com for consulting assistance with the following topics:\n\nDemographic analysis / Census data\nBusiness and location intelligence\nGeographic Information Systems (GIS) and spatial data science\nTraining in the R and Python programming languages\n\n\n\n\n\n\n\n  \n    \n      \n      Expert demographics, Census, and data science advisory\n    \n  \n  \n    \n      \n      Sophisticated location intelligence solutions for your business\n    \n  \n  \n    \n      \n      R and GIS training from the developer of the tidycensus R package\n    \n  \n  \n    \n      \n      Read the book Analyzing US Census Data: Methods, Maps, and Models in R\n    \n  \n\n\n\n\nTo receive on-going updates from Walker Data, consider signing up for the Walker Data mailing list:"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Register today for these upcoming online workshops:\nWorkshop 1: Analyzing the New 2020 Decennial Census Data\nJune 14, 2023, 11:00am-1:30pm Central Time (US)\nIn this workshop, you’ll learn how to analyze the 2020 Decennial Census’s Demographic and Housing Characteristics file using R and the tidycensus R package. Topics covered will include brand-new functionality in tidycensus to acquire and process data from the 2020 DHC file to help you analyze the data right away. You’ll gain experience with the tidyverse ecosystem of tools to reveal insights in the new data, and you’ll learn how to create stunning data visualizations to communicate those insights.\n\n    Register here\n\n\n\nWorkshop 2: Mapping and Spatial Analysis with the 2020 Decennial Census\nJune 21, 2023, 11:00am-1:30pm Central Time (US)\nMaps are powerful tools for communicating insights in the 2020 DHC data. In this workshop, you’ll learn how to use R and tidycensus to get spatial Census data seamlessly, and visualize the new data with a range of cartographic tools. You’ll also learn all about Census geography and how to perform powerful spatial analyses with 2020 Census data.\n\n    Register here\n\n\nWorkshop participants get the following materials: * Unlimited access to the workshop video * A custom tutorial on working with Census data not posted anywhere else * 30 minutes live Q&A with Kyle Walker, the developer of the tidycensus R package\nFor updates on these workshops and more data analysis content, subscribe to the Walker Data mailing list:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWALKER DATA also offers custom data science workshops that can be tailored to the needs of your organization. Please reach out to kyle@walker-data.com to discuss!"
  },
  {
    "objectID": "posts/pygris-arcgis/index.html",
    "href": "posts/pygris-arcgis/index.html",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "",
    "text": "Last week, I gave a workshop on working with geographic data in Python with the University of Michigan’s Social Science Data Analysis Network. The workshop focused on pygris, my new Python package for working with US Census Bureau geographic data resources. I was asked by multiple attendees if pygris works within ArcGIS Pro Python notebooks. I did not know the answer at the time, but it seemed like pygris - and other geospatial data packages in Python - combined with ArcGIS Pro could be quite powerful. I tested this process out, and it turns out that pygris and ArcGIS Pro work quite well together! Read on for a tutorial for how to use pygris (and your other favorite Python packages) in ArcGIS Pro.\nThis post was tested on ArcGIS Pro version 3.1."
  },
  {
    "objectID": "posts/pygris-arcgis/index.html#setting-up-a-new-conda-environment",
    "href": "posts/pygris-arcgis/index.html#setting-up-a-new-conda-environment",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "Setting up a new conda environment",
    "text": "Setting up a new conda environment\nArcGIS Pro ships with conda, a popular package manager for scientific Python. The default conda environment used by ArcGIS Pro is called arcgispro-py3, which has many of the most popular Python libraries for scientific computing and all the Python libraries needed for ArcGIS Pro installed. While this handles many use-cases within ArcGIS Pro, you can’t install new packages into this environment outside of an approved list. This means that you wouldn’t be able to use newer packages like pygris without first creating a new environment.\nWe’ll want our new environment to include all of the packages already found in arcgispro-py3, so we’ll need to clone it. You’ll want to click the Start icon in Windows then click “All apps”. Navigate to “ArcGIS” and select the Python command prompt.\n\nFrom the command prompt, enter the following command:\nconda create --name YOUR_ENV_NAME --clone arcgispro-py3\nwhere YOUR_ENV_NAME is the chosen name of your new environment. I’m calling my environment pygris, as I want to use it with the pygris package. After the environment is set up, you can install packages into it using conda or pip. As pygris is not yet available on conda, we install it with the following command:\npip install pygris\nThis will install pygris into your new conda environment along with all required dependencies."
  },
  {
    "objectID": "posts/pygris-arcgis/index.html#using-your-new-environment-in-arcgis-pro",
    "href": "posts/pygris-arcgis/index.html#using-your-new-environment-in-arcgis-pro",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "Using your new environment in ArcGIS Pro",
    "text": "Using your new environment in ArcGIS Pro\nTo use your new environment in ArcGIS Pro, you’ll first need to activate it - but not from the command line. Launch ArcGIS Pro and click Settings > Package Manager. In the upper-right corner of your screen, change the active environment to your new conda environment, e.g. “pygris”.\n\nLaunch a new project (mine is called pygris-blog), then launch a new ArcGIS Python Notebook by clicking Insert > New Notebook. I’ve organized my panes from left to right to show Contents, Notebook, Map, then Catalog.\n\nYou should now be able to start using pygris and any other of your favorite Python packages (that are installed) in ArcGIS just as you would in a regular Jupyter notebook. I’m running the following code from last week’s workshop to grab 2017-2021 ACS data on the percent of the population age 25+ with a bachelor’s degree for New York City and erasing water area from Census tracts obtained with pygris. Note that I’m using a coordinate reference system in .to_crs() appropriate for New York City, “NAD83(2011) / New York Long Island” which has the EPSG code 6538. Your project may require a different CRS for your area.\nfrom pygris import tracts\nfrom pygris.utils import erase_water\nfrom pygris.data import get_census\n\nnyc_counties = [\"New York\", \"Bronx\", \"Richmond\", \n                \"Kings\", \"Queens\"]\n\nnyc_tracts = tracts(state = \"NY\", county = nyc_counties,\n                    year = 2021, cache = True).to_crs(6538)\n\nny_college = get_census(dataset = \"acs/acs5/profile\",\n                        variables = \"DP02_0068PE\",\n                        year = 2021,\n                        params = {\n                          \"for\": \"tract:*\",\n                          \"in\": \"state:36\"},\n                        guess_dtypes = True,\n                        return_geoid = True)\n                        \nnyc_merged = nyc_tracts.merge(ny_college, how = \"inner\", on = \"GEOID\")\n\nnyc_erase = erase_water(nyc_merged, area_threshold = 0.9)"
  },
  {
    "objectID": "posts/pygris-arcgis/index.html#mapping-python-generated-data-with-arcgis",
    "href": "posts/pygris-arcgis/index.html#mapping-python-generated-data-with-arcgis",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "Mapping Python-generated data with ArcGIS",
    "text": "Mapping Python-generated data with ArcGIS\nIn a typical Python workflow, we might map our data with the .plot() method. We can do this directly in the notebook within ArcGIS Pro (note that we need %matplotlib inline and to use plt.show() to get this to work in an ArcGIS notebook). A major reason for linking pygris with ArcGIS Pro, however, is to gain access to ArcGIS Pro’s rich cartographic functionality. This will require a couple extra steps.\nWe’ll need to use the GeoAccessor class, which lives inside the ArcGIS API for Python. GeoAccessor allows us to interact with Pandas DataFrames and GeoPandas GeoDataFrames like our New York City object. Note that this was buggy prior to ArcGIS for Python version 2.0.1, so you’ll need to update the package if you have an older version.\nfrom arcgis.features import GeoAccessor\n\nnyc_arcgis = GeoAccessor.from_geodataframe(nyc_proj, column_name = \"geometry\")\nThe object nyc_arcgis is now a “spatially enabled data frame” that can interact with the ArcGIS suite of projects. ArcGIS Online users can publish the dataset as a feature layer with the to_featurelayer() method. Since we are using ArcGIS Pro, we will want to use to_featureclass() and write the layer to our project’s geodatabase. This process is very smooth; you would replace out_path with the path to your geodatabase.\nout_path = r\"C:\\Users\\kylewalker\\Documents\\ArcGIS\\Projects\\pygris-blog\\pygris-blog.gdb\\nyc_tract_education\"\n\nnyc_arcgis.spatial.to_featureclass(out_path)\nIn the Catalog pane, navigate to your project geodatabase. The new feature class (I’ve called mine nyc_tract_education should now be written there. Drag and drop it onto your map, and start exploring and mapping with ArcGIS’s cartography tools!"
  },
  {
    "objectID": "posts/lodes-commutes/index.html",
    "href": "posts/lodes-commutes/index.html",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "",
    "text": "In Chapter 11 of my book Analyzing US Census Data, I explore a sampling of the variety of government datasets that are available for the United States. One of the most useful of these datasets is LODES (LEHD Origin-Destination Employment Statistics). LODES is a synthetic dataset that represents, down to the Census block level, job counts by workplace and residence as well as the flows between them.\nGiven that LODES data are tabulated at the Census block level, analysts will often want to merge the data to Census geographic data like what is accessible in the pygris package. pygris includes a function, get_lodes(), that is modeled after the excellent lehdr R package by Jamaal Green, Dillon Mahmoudi, and Liming Wang.\nThis post will illustrate how to analyze the origins of commuters to the Census tract containing Apple’s headquarters in Cupertino, CA. In doing so, I’ll highlight some of the data wrangling utilities in pandas that allow for the use of method chaining, and show how to merge data to pygris shapes for mapping. The corresponding section in Analyzing US Census Data to this post is “Analyzing labor markets with lehdr.”"
  },
  {
    "objectID": "posts/lodes-commutes/index.html#acquiring-and-wrangling-lodes-data",
    "href": "posts/lodes-commutes/index.html#acquiring-and-wrangling-lodes-data",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "Acquiring and wrangling LODES data",
    "text": "Acquiring and wrangling LODES data\nTo get started, let’s import the functions and modules we need and give get_lodes() a try. get_lodes() requires specifying a state (as state abbreviation) and year; we are getting data for California in 2019, the most recent year currently available. The argument lodes_type = \"od\" tells pygris to get origin-destination flows data, and cache = True will download the dataset (which is nearly 100MB) to a local cache directory for faster use in the future.\n\nfrom pygris import tracts \nfrom pygris.data import get_lodes\nimport matplotlib.pyplot as plt\n\nca_lodes_od = get_lodes(\n    state = \"CA\",\n    year = 2019,\n    lodes_type = \"od\",\n    cache = True\n)\n\nca_lodes_od.head()\n\n\n\n\n\n  \n    \n      \n      w_geocode\n      h_geocode\n      S000\n      SA01\n      SA02\n      SA03\n      SE01\n      SE02\n      SE03\n      SI01\n      SI02\n      SI03\n      createdate\n    \n  \n  \n    \n      0\n      060014001001007\n      060014001001044\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      20211018\n    \n    \n      1\n      060014001001007\n      060014001001060\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      20211018\n    \n    \n      2\n      060014001001007\n      060014038002002\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      20211018\n    \n    \n      3\n      060014001001007\n      060014041021003\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      20211018\n    \n    \n      4\n      060014001001007\n      060014042002012\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      20211018\n    \n  \n\n\n\n\nThe loaded dataset, which has nearly 16 million rows, represents synthetic origin-destination flows from Census block to Census block in California in 2019. Columns represent the Census block GEOIDs for both workplace and residence, as well as job counts for flows between them. S000 represents all jobs; see the LODES documentation for how other breakouts are defined.\n16 million rows is a lot of data to deal with all at once, so we’ll want to do some targeted data wrangling to make this more manageable. We’ll do so using a method chain, which is my preferred way to do data wrangling in Python given that I come from an R / tidyverse background. The code takes the full origin-destination dataset, rolls it up to the Census tract level, then returns (by Census tract) the number of commuters to Apple’s Census tract in Cupertino.\n\napple = (\n    ca_lodes_od\n    .assign(w_tract = ca_lodes_od['w_geocode'].str.slice(stop = 11),\n            h_tract = ca_lodes_od['h_geocode'].str.slice(stop = 11))\n    .query('w_tract == \"06085508102\"')\n    .groupby('h_tract', as_index = False)\n    .agg({'S000': sum})\n    .rename({'S000': 'apple_workers'}, axis = 1)\n)\n\napple.head()\n\n\n\n\n\n  \n    \n      \n      h_tract\n      apple_workers\n    \n  \n  \n    \n      0\n      06001400100\n      3\n    \n    \n      1\n      06001400200\n      3\n    \n    \n      2\n      06001400300\n      3\n    \n    \n      3\n      06001400400\n      2\n    \n    \n      4\n      06001400500\n      4\n    \n  \n\n\n\n\nLet’s step through how we did this:\n\nThe .assign() method is used to calculate two new Census tract columns. A great thing about Census GEOIDs is that child geographies (like Census blocks) contain information about parent geographies. In turn, we can calculate Census tract GEOIDs by slicing block GEOIDs for the first 11 characters.\n\n.query() is used to subset our data. We only want rows representing commuters to the Apple campus (or the area around it), so we query for that specific tract ID.\nNext, we’ll roll up our data to the tract level. We’ll first group the data by home Census tract with .groupby(), then calculate group sums with .agg().\n\nFinally, we use a dictionary passed to .rename() to give the jobs column a more interpretable name.\n\nNext, we’ll repeat this process to tabulate the total number of workers by home Census tract to be used as a denominator. After that, we can merge the Apple-area commuters dataset back in, and calculate a rate per 1000. Note the lambda notation used in the final step of the method chain: this allows us to refer to the dataset that is being created by the chain.\n\napple_commuters = (\n    ca_lodes_od\n    .assign(h_tract = ca_lodes_od['h_geocode'].str.slice(stop = 11))\n    .groupby('h_tract', as_index = False)\n    .agg({'S000': sum})\n    .rename({'S000': 'total_workers'}, axis = 1)\n    .merge(apple, on = 'h_tract')\n    .assign(apple_per_1000 = lambda x: 1000 * (x['apple_workers'] / x['total_workers']))\n)\n\napple_commuters.head()\n\n\n\n\n\n  \n    \n      \n      h_tract\n      total_workers\n      apple_workers\n      apple_per_1000\n    \n  \n  \n    \n      0\n      06001400100\n      1412\n      3\n      2.124646\n    \n    \n      1\n      06001400200\n      1095\n      3\n      2.739726\n    \n    \n      2\n      06001400300\n      2650\n      3\n      1.132075\n    \n    \n      3\n      06001400400\n      2341\n      2\n      0.854336\n    \n    \n      4\n      06001400500\n      2147\n      4\n      1.863065"
  },
  {
    "objectID": "posts/lodes-commutes/index.html#mapping-commute-flows-to-apple-headquarters",
    "href": "posts/lodes-commutes/index.html#mapping-commute-flows-to-apple-headquarters",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "Mapping commute flows to Apple headquarters",
    "text": "Mapping commute flows to Apple headquarters\nThe main purpose of the pygris package is to make the acquisition of US Census Bureau spatial data easy for Python users. Given that we have aggregated our data at the Census tract level, we can use the tracts() function to grab Census tract shapes for six counties in the San Francisco Bay Area. We’ll use the Cartographic Boundary shapefiles with cb = True to exclude most water area, and make sure to specify year = 2019 to match the 2019 LODES data.\n\nbay_tracts = tracts(state = \"CA\", cb = True,\n                    county = [\"San Francisco\", \"Alameda\", \"San Mateo\",\n                              \"Santa Clara\", \"Marin\", \"Contra Costa\"], \n                    year = 2019, cache = True)\n                    \nbay_tracts.plot()\n\nUsing FIPS code '06' for input 'CA'\n\n\nUsing FIPS code '075' for input 'San Francisco'\nUsing FIPS code '001' for input 'Alameda'\nUsing FIPS code '081' for input 'San Mateo'\nUsing FIPS code '085' for input 'Santa Clara'\nUsing FIPS code '041' for input 'Marin'\nUsing FIPS code '013' for input 'Contra Costa'\n\n\n<AxesSubplot:>\n\n\n\n\n\nWith our tracts in hand, we use the .merge() method to merge the tabulated LODES data to the Census tract shapes, then make a map with geopandas’ plotting functionality.\n\napple_bay = bay_tracts.merge(apple_commuters, left_on = \"GEOID\", right_on = \"h_tract\",\n                             how = \"left\")\n\napple_bay.fillna(0, inplace = True)\n\napple_bay.plot(column = 'apple_per_1000', legend = True, \n               cmap = \"cividis\", figsize = (8, 8), \n               k = 7, scheme = \"naturalbreaks\",\n               legend_kwds = {\"loc\": \"lower left\"})\n\nplt.title(\"Apple-area commuters (rate per 1000 total commuters)\\n2019 LODES data, Bay Area Census tracts\", fontsize = 12)\n\nax = plt.gca()\n\nax.set_axis_off()\n\n\n\n\nCommuters to Apple’s tract tend to be concentrated around that tract; however, several neighborhoods in San Francisco proper send dozens of commuters per 1000 total commuters south to Cupertino.\nThis is where the LODES section of my book chapter ends; however, a static map like this can be difficult to interpret for those less familiar with the Bay Area. The geopandas .explore() method can make this map interactive for exploration without much more code. We’ll also use the built-in Census geocoding interface in pygris to add a marker where Apple Park’s visitors center is located.\n\nfrom pygris.geocode import geocode\n\napple_bay_sub = apple_bay.filter(['GEOID', 'total_workers',\n                                  'apple_workers', 'apple_per_1000',\n                                  'geometry'])\n\nvisitor_center = geocode(\"10600 N Tantau Ave, Cupertino, CA 95014\",\n                         as_gdf = True)\n\nm = apple_bay_sub.explore(column = \"apple_per_1000\", cmap = \"cividis\",\n                          k = 7, scheme = \"naturalbreaks\", popup = True, \n                          tooltip = False,                   \n                          tiles = \"CartoDB positron\", \n                          style_kwds = {\"weight\": 0.5},\n                          legend_kwds = {\"caption\": \"Apple-area commuters per 1000\",\n                                          \"colorbar\": False},\n                          popup_kwds = {\"aliases\": ['Census tract', 'Total workers',\n                                                    'Apple-area commuters', 'Rate per 1000']})\n\nvisitor_center.explore(m = m, marker_type = \"marker\", tooltip = False)      \n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nIf you’ve found this post useful, follow along on Twitter, LinkedIn, or subscribe to my newsletter for more examples of how to translate topics from my book to Python in advance of the book’s release next month!"
  },
  {
    "objectID": "posts/proximity-analysis/index.html",
    "href": "posts/proximity-analysis/index.html",
    "title": "Distance and proximity analysis in Python",
    "section": "",
    "text": "Spatial data science projects frequently require the calculation of proximity to resources. Analysts in fields like health care, real estate, retail, education, and more are commonly tasked with finding out what resources are near to a given location, and potentially develop strategies to fill identified gaps in resource access. In Chapter 7 of my book Analyzing US Census Data, I illustrate a workflow that shows how to analyze accessibility from Census tracts to major trauma hospitals in Iowa. In this post, I’ll show you how to reproduce the first part of that analysis in Python for the neighboring state of South Dakota.\nSimilar to the section in my book, this post will cover how to calculate proximity in two ways: using straight-line (Euclidean) distances in a projected coordinate system, and using driving times derived from a hosted navigation service. You’ll learn how to compute distances using built-in tools in geopandas, and also use the routingpy Python package to interact with Mapbox’s Navigation APIs."
  },
  {
    "objectID": "posts/proximity-analysis/index.html#getting-and-formatting-location-data",
    "href": "posts/proximity-analysis/index.html#getting-and-formatting-location-data",
    "title": "Distance and proximity analysis in Python",
    "section": "Getting and formatting location data",
    "text": "Getting and formatting location data\nTo get started, we’ll need to acquire and format data on both Census tracts and hospitals. We can get Census tract boundaries for South Dakota using my pygris package; we’ll then use the .to_crs() method to transform to an appropriate projected coordinate reference system for the area.\nThe hospitals can be downloaded from the US Department of Homeland Security’s Homeland Infrastructure Foundation-Level Data (HIFLD) portal. HIFLD includes open, frequently-updated datasets on “critical infrastructure” such as schools, pharmacies, and health care facilities. After reading in the downloaded file, we subset the data using a regular expression and .str.contains() to include only Level I and Level II trauma centers, and drop any duplicated records.\n\nimport geopandas as gp\nfrom pygris import tracts\n\n# Coordinate system: State Plane South Dakota North\nsd_tracts = tracts(\"SD\", cb = True, \n                   year = 2021, \n                   cache = True).to_crs(6571)\n\n\nhospitals = gp.read_file('Hospitals.geojson').to_crs(6571)\n\ntrauma = hospitals.loc[hospitals['TRAUMA'].str.contains(\"LEVEL I\\\\b|LEVEL II\\\\b|RTH|RTC\")]\n\ntrauma = trauma.drop_duplicates(['ID'])\n\nUsing FIPS code '46' for input 'SD'\n\n\nThe next step is to identify those trauma hospitals that are near to South Dakota. While the hospitals dataset does have a STATE column, we don’t want to filter the dataset for only those hospitals that are located in the state. In some cases, the nearest trauma hospital might be in a different state, and we don’t want to exclude those from our analysis.\nThe approach here identifies all trauma hospitals within 100 kilometers of the South Dakota border, or within the state itself. We use a sequence of common GIS operations with geopandas to accomplish this. Our steps include:\n\nCombining the South Dakota Census tracts into a single shape with the .dissolve() method;\nDrawing a new shape that extends to 100km beyond the South Dakota border with the .buffer() method;\nUsing an inner spatial join to retain only those trauma centers that fall within the 100km buffer shape.\n\n\nsd_buffer = gp.GeoDataFrame(geometry = sd_tracts.dissolve().buffer(100000))\n\nsd_trauma = trauma.sjoin(sd_buffer, how = \"inner\")\n\nAfter running this operation, we can draw a quick plot to show the relationships between Census tracts and hospitals as in Section 7.4.1 of Analyzing US Census Data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize = (8, 5))\n\nsd_tracts.plot(ax = ax, color = \"grey\")\nsd_trauma.plot(ax = ax, color = \"red\")\n\n<AxesSubplot: >\n\n\n\n\n\nThere are seven Level I or Level II trauma centers within 100km of South Dakota (in fact, all are Level II). In fact, only three are actually in the state, with two in Sioux Falls and one in Rapid City. Three others are in North Dakota (Bismarck and Fargo) and one is just across the border in Sioux City, Iowa."
  },
  {
    "objectID": "posts/proximity-analysis/index.html#calculating-distances-to-trauma-centers",
    "href": "posts/proximity-analysis/index.html#calculating-distances-to-trauma-centers",
    "title": "Distance and proximity analysis in Python",
    "section": "Calculating distances to trauma centers",
    "text": "Calculating distances to trauma centers\nThe simplest way to calculate proximity is with straight-line distances. In a projected coordinate system, this amounts to little more than Euclidean geometry, and such distance calculations are readily available to us using the .distance() method in geopandas. Conceptually, we’ll want to think through how to represent polygon-to-point distances. The most accurate approach would likely be to find the population-weighted centroid of each Census tract using some underlying dataset like Census blocks. In the example here, I’m taking a more simplistic approach by finding the geographic centroid of each tract. The centroids are found in the centroid attribute of any polygon GeoDataFrame.\nOnce the centroids are identified, we can iterate over them with apply and build a dataset that represents a distance matrix between Census tracts and trauma hospitals. Distances are calculated in meters, the base unit of our coordinate reference system (State Plane South Dakota North).\n\ntract_centroids = sd_tracts.centroid\n\ndist = tract_centroids.geometry.apply(lambda g: sd_trauma.distance(g, align = False))\n\ndist.head()\n\n\n\n\n\n  \n    \n      \n      2321\n      4488\n      4489\n      4500\n      5453\n      5454\n      5455\n    \n  \n  \n    \n      0\n      179156.152185\n      422441.507300\n      422155.094660\n      313038.398120\n      490132.864570\n      62861.191081\n      61633.946651\n    \n    \n      1\n      315344.252959\n      276978.438522\n      276708.176024\n      254769.314501\n      384118.434400\n      208119.760220\n      206470.685056\n    \n    \n      2\n      275529.979201\n      354138.644057\n      353818.847981\n      213424.554461\n      495938.859489\n      157302.191011\n      156735.306982\n    \n    \n      3\n      187763.417249\n      408402.306426\n      408161.751845\n      362094.468043\n      419202.830653\n      107982.166667\n      105743.400331\n    \n    \n      4\n      655092.671279\n      344550.256147\n      344760.480163\n      605274.028698\n      87369.870363\n      592598.357613\n      590388.549939\n    \n  \n\n\n\n\nLet’s take a quick look at how distance to the nearest trauma center varies around South Dakota. We’ll use the .min() method to find the minimum distance to a hospital for each row, then divide the result by 1000 to convert the distances to kilometers. We can then draw a histogram to review the distribution.\n\nmin_dist = dist.min(axis = 'columns') / 1000\n\nsns.histplot(min_dist, binwidth = 10)\n\n<AxesSubplot: ylabel='Count'>\n\n\n\n\n\nOver 60 Census tracts are within 10km of a trauma center, reflecting tracts located within the population centers of Rapid City and Sioux Falls. However, many tracts in the state are beyond 100km from the nearest trauma center, with 26 200km or more away.\nWe know that in rural areas, however, straight-line distances can be misleading. Given the geography of highway networks, accessibility to a trauma center is mediated through accessibility to that road network. Let’s take a look at an alternative approach to calculating proximity to hospitals with a hosted routing service."
  },
  {
    "objectID": "posts/proximity-analysis/index.html#finding-travel-times-to-trauma-centers",
    "href": "posts/proximity-analysis/index.html#finding-travel-times-to-trauma-centers",
    "title": "Distance and proximity analysis in Python",
    "section": "Finding travel-times to trauma centers",
    "text": "Finding travel-times to trauma centers\nAnalysts who need to calculate proximity along a road network in Python have multiple options available to them. One way is to build a network using a tool like OSMnx; another way, shown here, is to connect to a hosted navigation service. In Analyzing US Census Data, I demonstrate how to use my mapboxapi R package to calculate a travel-time matrix between Census tracts and hospitals. In Python, we can connect to Mapbox’s navigation services with the routingpy package, an interface to several hosted navigation APIs.\nIn routingpy, Mapbox’s web services are referenced as MapboxOSRM. We’ll need a Mapbox account and access token to access these services; I’m storing mine in a variable named mapbox_key. I then initalize a connection to Mapbox with MapboxOSRM(), which I am calling mb.\n\nfrom routingpy.routers import MapboxOSRM\n\nmb = MapboxOSRM(api_key = mapbox_key)\n\nIn the mapboxapi R package, the processing required to prepare datasets for the Mapbox APIs is taken care of internally. To calculate a travel-time matrix in Python, we’ll need to do some data processing to format our Census tracts and hospitals in the correct way before we send them to the routing service.\nBelow, I’m defining a function points_to_coords() to convert a given GeoDataFrame of points to a list of lists, with each list element representing the XY coordinates of a given input point in the dataset. We’ll then call the function on the Census tract centroids and the trauma hospitals datasets.\n\n# Function to convert points to coordinates\ndef points_to_coords(input):\n  geom = input.to_crs(4326).geometry\n\n  return [[g.x, g.y] for g in geom]\n\n# Generate list of coordinates\ntract_coords = points_to_coords(tract_centroids)\nhospital_coords = points_to_coords(sd_trauma)\n\nNext, we’ll set up some custom code that allows for the creation of a 242 by 7 travel-time matrix. This is necessary because Mapbox’s Matrix API only allows for a maximum of 25 coordinate pairs per request, and we have 249 total! Let’s run the code, then walk through the steps we took.\n\nimport pandas as pd\n\nsplit_size = 25 - len(hospital_coords)\n\nchunks = [tract_coords[x:x + split_size] for x in range(0, len(tract_coords), split_size)]\n\ntimes_list = []\n\nfor chunk in chunks:\n\n  all_coords = chunk + hospital_coords\n\n  # Find the indices of origin and destination\n  origin_ix = [x for x in range(0, len(chunk), 1)]\n  hospital_ix = [y for y in range(len(chunk), len(all_coords), 1)]\n\n  # Run the travel-time matrix\n  times = mb.matrix(locations = all_coords,\n                    profile = \"driving\",\n                    sources = origin_ix,\n                    destinations = hospital_ix)\n\n  # Convert to a dataframe\n  times_df = pd.DataFrame(times.durations)\n\n  times_list.append(times_df)\n\nall_times = pd.concat(times_list, ignore_index = True) \n\nall_times.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n    \n  \n  \n    \n      0\n      7888.9\n      22103.4\n      22233.8\n      11812.5\n      19938.0\n      3821.1\n      3672.0\n    \n    \n      1\n      14748.0\n      14480.9\n      14611.3\n      12718.5\n      17358.1\n      10649.7\n      10531.1\n    \n    \n      2\n      10251.6\n      18023.1\n      18153.5\n      7747.9\n      22173.1\n      6183.8\n      6034.7\n    \n    \n      3\n      8646.7\n      20428.2\n      20558.6\n      15953.4\n      15610.0\n      4548.4\n      4429.8\n    \n    \n      4\n      26589.6\n      17174.6\n      17177.2\n      27658.8\n      3819.9\n      22491.3\n      22372.7\n    \n  \n\n\n\n\nYou can interpret the code above as follows:\n\nGiven that we have far more coordinates than the Matrix API’s limit of 25, we need to split up our origin coordinates into chunks. We identify a split_size as 25 minus the number of hospitals (7); this is 18. We then split the Census tract centroid coordinates into chunks of 18 using a list comprehension.\nNext, we initialize a list to store our chunked output, and iterate through the chunks. This process involves the following:\n\nWe’ll combine the list of origin coordinates (for a given chunk) and the hospital coordinates in the object all_coords;\nNext, we identify the indices of the origin coordinates and the hospital coordinates in all_coords. This is critical information that we’ll need to pass along to the Mapbox Matrix API.\n\nWe use mb.matrix() to make a request to the Matrix API. The request requires a list of coordinates; a travel profile; and the source and destination indices.\n\nOnce we get the results back, we’ll create a pandas DataFrame and append to times_list.\n\n\nFinally, we use pd.concat() to combine the chunked results into a single DataFrame, which you see above. Values represent the drive-time in seconds between Census tract centroids and the trauma hospitals.\n\nOur last steps before plotting involve finding the minimum travel-time from each Census tract to a trauma hospital, converting to minutes, then adding back to the Census tracts GeoDataFrame as a column named 'time'. Using the GeoDataFrame method .plot() with some customization, we can reproduce the plot in Analyzing US Census Data.\n\nmin_times = all_times.min(axis = \"columns\") / 60\n\nsd_tracts['time'] = min_times\n\nsd_tracts.plot(column = \"time\", legend = True, figsize = (8, 5),\n               cmap = \"magma\",\n               legend_kwds = {\"location\": \"top\",\n                              \"shrink\": 0.5})\n\nplt.title(\"Travel-time (minutes) to nearest\\nLevel I or Level II trauma hospital\\n\\n\\n\\n\")\n\nax = plt.gca()\n\nax.set_axis_off()\n\nax.annotate('Census tracts in South Dakota\\nData sources: US Census Bureau, US DHS, Mapbox', \n            xy=(0.1, 0.1), xycoords='figure fraction',\n            fontsize=8, ha='left', va='top')\n\nText(0.1, 0.1, 'Census tracts in South Dakota\\nData sources: US Census Bureau, US DHS, Mapbox')\n\n\n\n\n\nThe map shows distinct gaps in accessibility to Level II trauma centers across the state of South Dakota. While neighborhoods in Rapid City, Sioux Falls, and the southeastern corner of the state near Sioux City are a short drive from a trauma hospital, some central South Dakota Census tracts are more than three and a half hours away from the nearest Level II trauma hospital by car.\nIf you’ve found this post useful, consider picking up a copy of my book Analyzing US Census Data: Methods, Maps, and Models in R, which hits shelves next month. I’ll keep sharing Python spatial data science workflows on this blog as well, so keep an eye out for future posts!"
  },
  {
    "objectID": "posts/esda-with-python/index.html",
    "href": "posts/esda-with-python/index.html",
    "title": "Exploratory spatial data analysis with Python",
    "section": "",
    "text": "In early 2023, the print copy of my book Analyzing US Census Data: Methods, Maps, and Models in R will be available for purchase. The response to the free online version of the book has been fantastic thus far. One question I commonly get asked, however, is “will you re-produce this for Python? I’d love to use this work but I don’t use R.”\nI don’t have plans to replicate all of my R work in Python, but I did get the itch in the second half of 2022 to learn Python package development. The result is pygris, which is a port of the R tigris package but with some additional features.\nTo celebrate the publication of my book as well as the pygris package, I’m launching a blog series to illustrate how to reproduce some of my favorite examples from my book in Python. Each example will feature pygris. Follow along - I hope you find it useful!"
  },
  {
    "objectID": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "href": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "title": "Exploratory spatial data analysis with Python",
    "section": "Mapping Local Moran’s I (LISA) clusters in Python",
    "text": "Mapping Local Moran’s I (LISA) clusters in Python\nCorresponding section in Analyzing US Census Data: Identifying clusters and spatial outliers with local indicators of spatial association (LISA)\nI came into graduate school intending to be a qualitative researcher, but I really got excited about spatial data analysis (and changed my career trajectory) when I learned GeoDa, a GUI tool for exploratory spatial data analysis. The method in GeoDa that resonated with me the most was the local form of the Moran’s I, an example of a LISA (local indicators of spatial association) statistic.\nLISAs are exploratory tools that help you make sense of spatial patterns in a dataset. They help surface preliminary answers to these questions:\n\nWhere are there concentrations of high attribute values in my spatial dataset?\nConversely, where can I find concentrations of low attribute values in my data?\nFinally, are there any unexpected values in my dataset, given the characteristics of their neighbors? These “spatial outliers” can be above-average values surrounded by low values, or below-average values surrounded by high values.\n\nThis post will walk you through how to create an interactive LISA map of median age by Census tract from the 2017-2021 American Community Survey, similar to the example in Section 7.7.3 of my book. That section of my book covers more technical details about LISA if you are interested in reading further.\n\nGetting the data with pygris\nTo get started, we’ll use pygris to get the data required for our analysis. The core functionality in pygris is a suite of functions to return US Census Bureau TIGER/Line shapefiles as GeoPandas GeoDataFrames.\nLet’s import the tracts() function to demonstrate how this works for the Minneapolis-St. Paul, Minnesota area. We’ll define a list of the seven core Twin Cities counties, and request Census tract boundaries for those counties with tracts(). pygris functions translate state names/abbreviations and county names internally to FIPS codes, so there is no need to look them up.\nThe argument year = 2021 gives back the 2021 version of the Census tract boundaries, which will be important as we’ll be matching to corresponding 2021 ACS data. Finally, the argument cache = True stores the downloaded shapefile in a local cache, which means that I won’t need to download it again from the Census website in future projects.\n\nfrom pygris import tracts\n\ntc_counties = [\"Hennepin\", \"Ramsey\", \"Scott\", \n               \"Carver\", \"Dakota\", \"Washington\", \"Anoka\"]\n\ntc_tracts = tracts(state = \"MN\", county = tc_counties, \n                   year = 2021, cache = True)\n\ntc_tracts.plot()\n\nUsing FIPS code '27' for input 'MN'\n\n\nUsing FIPS code '053' for input 'Hennepin'\nUsing FIPS code '123' for input 'Ramsey'\nUsing FIPS code '139' for input 'Scott'\nUsing FIPS code '019' for input 'Carver'\nUsing FIPS code '037' for input 'Dakota'\nUsing FIPS code '163' for input 'Washington'\nUsing FIPS code '003' for input 'Anoka'\n\n\n<AxesSubplot:>\n\n\n\n\n\nWe’ll next need to grab data on median age and merge to the Census tract shapes. I don’t have plans to implement my R package tidycensus in Python; tidycensus is designed specifically for use within R’s tidyverse and Pythonic interfaces to the Census API like cenpy already exist. However, I’ve always admired Hannah Recht’s work on the R censusapi package, which can connect to all Census Bureau API endpoints. pygris includes a get_census() function inspired by censusapi that developers can use to build interfaces to the data they need.\nLet’s use get_census() to get data on median age at the Census tract level for Minnesota, then merge to our Census tracts for additional analysis.\n\nfrom pygris import validate_state\nfrom pygris.data import get_census\n\nmn_median_age = get_census(dataset = \"acs/acs5\",\n                           variables = \"B01002_001E\",\n                           year = 2021,\n                           params = {\n                             \"for\": \"tract:*\",\n                             \"in\": f\"state:{validate_state('MN')}\"},\n                           guess_dtypes = True,\n                           return_geoid = True\n)\n\nmn_median_age.head()\n\nUsing FIPS code '27' for input 'MN'\n\n\n\n\n\n\n  \n    \n      \n      B01002_001E\n      GEOID\n    \n  \n  \n    \n      1\n      57.8\n      27001770100\n    \n    \n      2\n      51.5\n      27001770200\n    \n    \n      3\n      46.8\n      27001770300\n    \n    \n      4\n      57.7\n      27001770401\n    \n    \n      5\n      49.1\n      27001770402\n    \n  \n\n\n\n\nYou can understand the arguments to get_census() as follows:\n\ndataset is the dataset name on the Census API you are connecting to. Datasets can be found at https://api.census.gov/data.html in the “Dataset Name” column.\nvariables is a string (or list of strings) representing the variable IDs you want for a given dataset. For the 2021 5-year ACS, those variable IDs are found at https://api.census.gov/data/2021/acs/acs5/variables.html.\n\nyear is the year of your data (or end-year for a 5-year ACS sample); the Census API will refer to this as the “vintage” of the data.\nparams is a dict of query parameters to send to the API. Each endpoint will have its own parameters, so you’ll need to spend a little time with the Census API documentation to learn what you can use. In our case, we are requesting data for Census tracts in Minnesota. The built-in validate_state() function can be used here to convert 'MN' to an appropriate FIPS code.\n\nguess_dtypes and return_geoid are convenience parameters that you’ll want to use judiciously. guess_dtypes tries to guess which columns to convert to numeric, and return_geoid tries to find columns to concatenate into a GEOID column that can be used for merging to Census shapes. These arguments won’t be appropriate for every API endpoint.\n\nWith our data in hand, we can do an inner merge and map the result:\n\nimport matplotlib.pyplot as plt\n\ntc_tract_age = tc_tracts.merge(mn_median_age, how = \"inner\", on = \"GEOID\")\n\ntc_tract_age.plot(column = \"B01002_001E\", legend = True)\n\nplt.title(\"Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS\")\n\nText(0.5, 1.0, 'Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS')\n\n\n\n\n\n\n\nAnalyzing spatial clustering with PySAL\nThe PySAL family of Python packages is central to the work of anyone who needs to analyze spatial data in Python. The esda package makes the calculation of the local Moran’s I statistic remarkably smooth. We will generate a Queen’s case spatial weights object (see my book for more technical details) to represent relationships between Census tracts and their neighbors, then call the Moran_Local() function to calculate the LISA statistics.\n\nfrom libpysal import weights\nimport esda\n\ntc_clean = tc_tract_age.copy().dropna().filter(['GEOID', 'B01002_001E', 'geometry'])\n\nw = weights.contiguity.Queen.from_dataframe(tc_clean)\n\nm = esda.Moran_Local(tc_clean['B01002_001E'], w, seed = 1983)\n\ntype(m)\n\nesda.moran.Moran_Local\n\n\nWe get an object of type Moran_Local which stores our analytical result. The documentation provides a comprehensive overview of the object’s attributes. We’ll grab two here and add them back to our Census tract dataset as new columns: q, which is the LISA quadrant (denoting the cluster type); and p_sim, which gives us a pseudo-p-value based on conditional permutation (see here for further discussion of this concept).\nUsing this information, we can identify “significant” and “non-significant” clusters and generate some more informative labels.\n\nimport numpy as np\n\n# We can extract the LISA quadrant along with the p-value from the lisa object\ntc_clean['quadrant'] = m.q\ntc_clean['p_sim'] = m.p_sim\n# Convert all non-significant quadrants to zero\ntc_clean['quadrant'] = np.where(tc_clean['p_sim'] > 0.05, 0, tc_clean['quadrant'])\n\n# Get more informative descriptions\ntc_clean['quadrant'] = tc_clean['quadrant'].replace(\n  to_replace = {\n    0: \"Not significant\",\n    1: \"High-high\",\n    2: \"Low-high\",\n    3: \"Low-low\",\n    4: \"High-low\"\n  }\n)\n\ntc_clean.head()\n\n\n\n\n\n  \n    \n      \n      GEOID\n      B01002_001E\n      geometry\n      quadrant\n      p_sim\n    \n  \n  \n    \n      0\n      27053103700\n      29.3\n      POLYGON ((-93.25825 44.98358, -93.25790 44.983...\n      Not significant\n      0.070\n    \n    \n      1\n      27053104100\n      28.2\n      POLYGON ((-93.31847 44.98174, -93.31847 44.983...\n      Low-low\n      0.043\n    \n    \n      2\n      27053104400\n      32.4\n      POLYGON ((-93.28158 44.97790, -93.28153 44.978...\n      Not significant\n      0.228\n    \n    \n      3\n      27053105100\n      44.6\n      POLYGON ((-93.32873 44.96012, -93.32873 44.960...\n      Not significant\n      0.127\n    \n    \n      4\n      27053105400\n      33.2\n      POLYGON ((-93.26972 44.96807, -93.26926 44.969...\n      Not significant\n      0.310\n    \n  \n\n\n\n\n\n\nBuilding an interactive LISA map\nWe now have all the information necessary to map LISA clusters. I’m going to show a workflow that differs slightly from typical LISA maps like the one illustrated in my book. One disadvantage of static LISA maps is that they assume an analyst has familiarity with the region under study. Without this familiarity, it can be difficult to determine exactly which locations are represented by different cluster types.\nEnter the .explore() GeoDataFrame method in GeoPandas. .explore() is an interface to Leaflet.js through Folium. Simply calling .explore() on a GeoDataFrame gets you started interactively exploring your spatial data; however, the method itself is a fairly full-featured interactive mapping engine.\nWith a little customization, we can build out an informative interactive map showing our LISA analysis of median age by Census tract in the Twin Cities. Here’s how we do it:\n\nWe choose \"quadrant\" as the column to visualize, and pass a list of colors to cmap to align with the typical color scheme used for LISA mapping (with some small modifications to improve visibility).\nlegend = True adds an informative legend, and a muted grey basemap is selected with tiles.\nThe various _kwds parameters are quite powerful, as this is how you will do more fine-grained customization of your map. We’ll reduce the line weight of our polygons to 0.5, and importantly do some customization of the popup to change the column names to informative aliases. Click on a Census tract to see what you get!\n\n\n# Build a LISA cluster map \ntc_clean.explore(column = \"quadrant\", \n                 cmap = [\"red\", \"hotpink\", \"deepskyblue\", \"blue\", \"lightgrey\"], \n                 legend = True, \n                 tiles = \"CartoDB positron\", \n                 style_kwds = {\"weight\": 0.5}, \n                 legend_kwds = { \"caption\": \"LISA quadrant\"}, \n                 tooltip = False, \n                 popup = True,\n                 popup_kwds = {\n                    \"aliases\": [\"GEOID\", \"Median age\", \"LISA quadrant\", \"Pseudo p-value\"]\n                 })\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nOur analytical result shows that younger areas tend to be found nearer to the Minneapolis / St. Paul urban cores, and older areas cluster in the western, southern, and northeastern suburbs. Spatial outliers are scattered throughout the region, and the map’s interactivity allows us to zoom in and click to understand these outliers in greater detail.\nTry out this workflow for yourself, and follow along here for more of my favorite examples from Analyzing US Census Data translated to Python over the next few months."
  },
  {
    "objectID": "posts/lodes-2020/index.html",
    "href": "posts/lodes-2020/index.html",
    "title": "Mapping jobs and commutes with 2020 LODES data and deck.gl",
    "section": "",
    "text": "Last month, version 8 of the LEHD Origin-Destination Employment Statistics (LODES) dataset was released. This long-awaited release includes data on workplaces, residences, and origin-destination flows for workers in 2020, along with a time series of these statistics back to 2002 enumerated at 2020 Census blocks.\nThe latest release of the pygris package for Python enables programmatic access to these new data resources with its get_lodes() function. This new release also allows you to request Census geometry or longitude / latitude coordinates along with your LODES data, making data visualization and mapping straightforward. Let’s try it out!"
  },
  {
    "objectID": "posts/lodes-2020/index.html#mapping-job-locations-by-census-block",
    "href": "posts/lodes-2020/index.html#mapping-job-locations-by-census-block",
    "title": "Mapping jobs and commutes with 2020 LODES data and deck.gl",
    "section": "Mapping job locations by Census block",
    "text": "Mapping job locations by Census block\nTo get started, let’s take care of some imports. We’ll be using the following:\n\nThe get_lodes() function in the pygris package gives us access to the brand-new LODES data. There is a lot more you can do with get_lodes(); review the package documentation for more examples.\n\npydeck is a Python interface to deck.gl, one of the most stunning data visualization libraries around. As you’ll see, deck.gl can help you create performant three-dimensional visualizations with large datasets.\n\nWe’ll also use matplotlib to do some custom color work for our maps.\n\nfrom pygris.data import get_lodes\nimport pydeck\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nThe first example will visualize the distribution of accommodation and food service workers by Census block in Kentucky. We can get this information from the LODES Worker Area Characteristics (WAC) dataset, which helps us understand the geography of jobs for small areas.\nThe latest version of pygris (0.1.5) includes some mapping helpers in get_lodes(). The new return_geometry parameter identifies the appropriate TIGER/Line shapefile to merge to the requested LODES data and returns a GeoPandas GeoDataFrame with geometry. An alternative approach, which we will be using here, uses the new return_lonlat parameter. This gives us a Pandas DataFrame with columns representing the centroid of the location. This representation of geography works quite well with deck.gl.\nLet’s get WAC data for the state of Kentucky in 2020.\nky_lodes_wac = get_lodes(\n  state = \"KY\", \n  year = 2020, \n  lodes_type = \"wac\",\n  cache = True,\n  return_lonlat = True\n)\nThe returned data have a host of columns representing jobs by category within that block, along with two additional columns, w_lon and w_lat, which represent the longitude and latitude of each block centroid.\nOur next step is to write a color-generating function to add some context to our visualization. For cartographers coming to deck.gl from other mapping libraries, color formatting can be tricky. deck.gl expects RGBA colors with values ranging from 0 to 255; while many mapping libraries translate column values to colors for you, we’ll need to do this manually.\nThe function, column_to_rgba(), normalizes an input column and converts it to a column where every element is a list of format [R, G, B, A] for a given color map cmap. We’ll use this function to add a column to our dataset, 'color', that is based on values in the CNS18 column (representing accommodation and food service jobs) and uses the viridis color palette.\ndef column_to_rgba(column, cmap, alpha):\n    normalized = (column - column.min()) / (column.max() - column.min())\n    my_cmap = plt.get_cmap(cmap)\n    colors = normalized.apply(lambda x: [int(i * 255) for i in mcolors.to_rgba(my_cmap(x, alpha = alpha))])\n\n    return colors\n  \n  \nky_lodes_wac['color'] = column_to_rgba(ky_lodes_wac['CNS18'], \"viridis\", 0.6)\nThe longitude / latitude data will work well for a deck.gl ColumnLayer. A column layer is a three-dimensional visualization that renders each location as a column, with height and color optionally scaled to a given characteristic in the dataset. This is a nice alternative to a choropleth map of jobs by block, as block polygons can be very irregular.\nlayer = pydeck.Layer(\n  \"ColumnLayer\",\n  ky_lodes_wac,\n  get_position=[\"w_lon\", \"w_lat\"],\n  auto_highlight=True,\n  elevation_scale=20,\n  pickable=True,\n  get_elevation = \"CNS18\",\n  get_fill_color = \"color\",\n  elevation_range=[0, 1000],\n  extruded=True,\n  coverage=1\n)\n\n# Set the viewport location\nview_state = pydeck.ViewState(\n  longitude=-85.4095567,\n  latitude=37.2086276,\n  zoom=6,\n  min_zoom=5,\n  max_zoom=15,\n  pitch=40.5,\n  bearing=-27.36\n)\n\ntooltip = {\"html\": \"Number of accommodation / food service jobs: {CNS18}\"}\n\n# Render\nr = pydeck.Deck(\n  layers=[layer], \n  initial_view_state=view_state, \n  map_style = \"light\", \n  tooltip = tooltip\n)\n\nr.to_html(\"ky_service.html\")\nBrowse the map and look for interesting patterns. Note how seamlessly deck.gl visualizes all 30,000 block locations in the dataset!"
  },
  {
    "objectID": "posts/lodes-2020/index.html#mapping-origin-destination-flows",
    "href": "posts/lodes-2020/index.html#mapping-origin-destination-flows",
    "title": "Mapping jobs and commutes with 2020 LODES data and deck.gl",
    "section": "Mapping origin-destination flows",
    "text": "Mapping origin-destination flows\nThe return_lonlat feature in get_lodes() also works great for representing origin-destination flows. The origin-destination dataset in LODES, acquired with lodes_type = \"od\", returns block-to-block flows for all home-to-work combinations in a given state.\nGiven that block-to-block flows could quickly get visually overwhelming, we may want to aggregate our data to a parent geography. Let’s acquire origin-destination flows for the state of Texas, and aggregate to the Census tract level with the argument agg_level = \"tract\".\ntx_od = get_lodes(\n  state = \"TX\", \n  year = 2020, \n  lodes_type=\"od\",\n  agg_level = \"tract\",\n  cache = True, \n  return_lonlat = True\n)\nThe data we get back includes h_lon and h_lat columns representing the centroid of the home Census tract, and w_lon and w_lat columns for the centroid of the work Census tract.\nWe’ll visualize these flows with a deck.gl ArcLayer; incidentally, the PyDeck documentation uses LODES data to show how ArcLayers work.\nLet’s refine the data first to answer a specific question. I live in Fort Worth, Texas, and a major growth area for the city is AllianceTexas, a fast-developing industrial and commercial corridor. We’ll generate a new object, top_commutes, that identifies those Census tracts sending at least 25 commuters to the Census tract containing the southern part of the Alliance airport.\ntop_commutes = tx_od.query('w_geocode == \"48439113932\" & S000 >= 25')\nFrom here, we can basically replicate the example from the PyDeck documentation, but apply it to commute flows to Alliance in Fort Worth.\nimport pydeck\n\nGREEN_RGB = [0, 255, 0, 200]\nRED_RGB = [240, 100, 0, 200]\n\narc_layer = pydeck.Layer(\n  \"ArcLayer\",\n  data=top_commutes,\n  get_width=\"S000 / 5\",\n  get_source_position=[\"h_lon\", \"h_lat\"],\n  get_target_position=[\"w_lon\", \"w_lat\"],\n  get_tilt=15,\n  get_source_color=RED_RGB,\n  get_target_color=GREEN_RGB,\n  pickable=True,\n  auto_highlight=True\n)\n\nview_state = pydeck.ViewState(\n  latitude=32.708664, \n  longitude=-97.360546, \n  bearing=45, \n  pitch=50, \n  zoom=8\n)\n\ntooltip = {\"html\": \"{S000} jobs <br /> Home of commuter in red; work location in green\"}\nr = pydeck.Deck(\n  arc_layer, \n  initial_view_state=view_state, \n  tooltip=tooltip, \n  map_style = \"road\"\n)\n\nr.to_html(\"alliance_commuters.html\")\nWe get a compelling origin-destination flow map showing the locations that sent the most commuters to AllianceTexas in 2020.\n\n\n\n\nWorking with LODES data can have massive benefits for your projects and your business. If you’d like to discuss how to integrate these insights into your work, please don’t hesitate to reach out!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "WALKER DATA is a consultancy helping clients solve a wide range of data science problems. We specialize in the following areas:\n\nSpatial data science and Geographic Information Systems (GIS)\nDemographics and location intelligence\nPredictive analytics and machine learning\nData visualization and dashboard development with tools like Shiny, Mapbox, and Tableau\nCoaching and training for the R and Python programming languages.\n\n\n\n\nBook a discovery call today!\n\n\nWALKER DATA is led by its principal, Kyle Walker, Ph.D. Kyle is an internationally-recognized researcher, consultant, and software developer in the field of spatial data science, and was the 2022 recipient of the Spatial Data Scientist of the Year award from the software company CARTO. He is the author of the book Analyzing US Census Data: Methods, Maps, and Models in R, and has published several popular software packages for spatial data science in R and Python that have been collectively downloaded over 1 million times.\nKyle also serves as Director of Research for the Linnaean Company, a data science and strategy firm helping companies with business and location intelligence solutions. Learn more about the Linnaean Company here."
  }
]