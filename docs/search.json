[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "WALKER DATA",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nAnalyzing labor markets in Python with LODES data\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nExploratory spatial data analysis with Python\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nKyle Walker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GIS, demographics, and data science consulting",
    "section": "",
    "text": "Welcome to Walker Data, the data science consulting website of Kyle Walker. Please reach out to kyle@walker-data.com for consulting assistance with the following topics:\n\nDemographic analysis / Census data\nBusiness and location intelligence\nGeographic Information Systems (GIS) and spatial data science\nTraining in the R and Python programming languages\n\n\n\n\n\n\n\n  \n    \n      \n      Expert demographics, Census, and data science advisory\n    \n  \n  \n    \n      \n      Sophisticated location intelligence solutions for your business\n    \n  \n  \n    \n      \n      R and GIS training from the developer of the tidycensus R package\n    \n  \n  \n    \n      \n      Read the book Analyzing US Census Data: Methods, Maps, and Models in R\n    \n  \n\n\n\n\nTo receive on-going updates from Walker Data, consider signing up for the Walker Data mailing list:"
  },
  {
    "objectID": "posts/esda-with-python/index.html",
    "href": "posts/esda-with-python/index.html",
    "title": "Exploratory spatial data analysis with Python",
    "section": "",
    "text": "In early 2023, the print copy of my book Analyzing US Census Data: Methods, Maps, and Models in R will be available for purchase. The response to the free online version of the book has been fantastic thus far. One question I commonly get asked, however, is “will you re-produce this for Python? I’d love to use this work but I don’t use R.”\nI don’t have plans to replicate all of my R work in Python, but I did get the itch in the second half of 2022 to learn Python package development. The result is pygris, which is a port of the R tigris package but with some additional features.\nTo celebrate the publication of my book as well as the pygris package, I’m launching a blog series to illustrate how to reproduce some of my favorite examples from my book in Python. Each example will feature pygris. Follow along - I hope you find it useful!"
  },
  {
    "objectID": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "href": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "title": "Exploratory spatial data analysis with Python",
    "section": "Mapping Local Moran’s I (LISA) clusters in Python",
    "text": "Mapping Local Moran’s I (LISA) clusters in Python\nCorresponding section in Analyzing US Census Data: Identifying clusters and spatial outliers with local indicators of spatial association (LISA)\nI came into graduate school intending to be a qualitative researcher, but I really got excited about spatial data analysis (and changed my career trajectory) when I learned GeoDa, a GUI tool for exploratory spatial data analysis. The method in GeoDa that resonated with me the most was the local form of the Moran’s I, an example of a LISA (local indicators of spatial association) statistic.\nLISAs are exploratory tools that help you make sense of spatial patterns in a dataset. They help surface preliminary answers to these questions:\n\nWhere are there concentrations of high attribute values in my spatial dataset?\nConversely, where can I find concentrations of low attribute values in my data?\nFinally, are there any unexpected values in my dataset, given the characteristics of their neighbors? These “spatial outliers” can be above-average values surrounded by low values, or below-average values surrounded by high values.\n\nThis post will walk you through how to create an interactive LISA map of median age by Census tract from the 2017-2021 American Community Survey, similar to the example in Section 7.7.3 of my book. That section of my book covers more technical details about LISA if you are interested in reading further.\n\nGetting the data with pygris\nTo get started, we’ll use pygris to get the data required for our analysis. The core functionality in pygris is a suite of functions to return US Census Bureau TIGER/Line shapefiles as GeoPandas GeoDataFrames.\nLet’s import the tracts() function to demonstrate how this works for the Minneapolis-St. Paul, Minnesota area. We’ll define a list of the seven core Twin Cities counties, and request Census tract boundaries for those counties with tracts(). pygris functions translate state names/abbreviations and county names internally to FIPS codes, so there is no need to look them up.\nThe argument year = 2021 gives back the 2021 version of the Census tract boundaries, which will be important as we’ll be matching to corresponding 2021 ACS data. Finally, the argument cache = True stores the downloaded shapefile in a local cache, which means that I won’t need to download it again from the Census website in future projects.\n\nfrom pygris import tracts\n\ntc_counties = [\"Hennepin\", \"Ramsey\", \"Scott\", \n               \"Carver\", \"Dakota\", \"Washington\", \"Anoka\"]\n\ntc_tracts = tracts(state = \"MN\", county = tc_counties, \n                   year = 2021, cache = True)\n\ntc_tracts.plot()\n\nUsing FIPS code '27' for input 'MN'\n\n\nUsing FIPS code '053' for input 'Hennepin'\nUsing FIPS code '123' for input 'Ramsey'\nUsing FIPS code '139' for input 'Scott'\nUsing FIPS code '019' for input 'Carver'\nUsing FIPS code '037' for input 'Dakota'\nUsing FIPS code '163' for input 'Washington'\nUsing FIPS code '003' for input 'Anoka'\n\n\n<AxesSubplot:>\n\n\n\n\n\nWe’ll next need to grab data on median age and merge to the Census tract shapes. I don’t have plans to implement my R package tidycensus in Python; tidycensus is designed specifically for use within R’s tidyverse and Pythonic interfaces to the Census API like cenpy already exist. However, I’ve always admired Hannah Recht’s work on the R censusapi package, which can connect to all Census Bureau API endpoints. pygris includes a get_census() function inspired by censusapi that developers can use to build interfaces to the data they need.\nLet’s use get_census() to get data on median age at the Census tract level for Minnesota, then merge to our Census tracts for additional analysis.\n\nfrom pygris import validate_state\nfrom pygris.data import get_census\n\nmn_median_age = get_census(dataset = \"acs/acs5\",\n                           variables = \"B01002_001E\",\n                           year = 2021,\n                           params = {\n                             \"for\": \"tract:*\",\n                             \"in\": f\"state:{validate_state('MN')}\"},\n                           guess_dtypes = True,\n                           return_geoid = True\n)\n\nmn_median_age.head()\n\nUsing FIPS code '27' for input 'MN'\n\n\n\n\n\n\n  \n    \n      \n      B01002_001E\n      GEOID\n    \n  \n  \n    \n      1\n      57.8\n      27001770100\n    \n    \n      2\n      51.5\n      27001770200\n    \n    \n      3\n      46.8\n      27001770300\n    \n    \n      4\n      57.7\n      27001770401\n    \n    \n      5\n      49.1\n      27001770402\n    \n  \n\n\n\n\nYou can understand the arguments to get_census() as follows:\n\ndataset is the dataset name on the Census API you are connecting to. Datasets can be found at https://api.census.gov/data.html in the “Dataset Name” column.\nvariables is a string (or list of strings) representing the variable IDs you want for a given dataset. For the 2021 5-year ACS, those variable IDs are found at https://api.census.gov/data/2021/acs/acs5/variables.html.\n\nyear is the year of your data (or end-year for a 5-year ACS sample); the Census API will refer to this as the “vintage” of the data.\nparams is a dict of query parameters to send to the API. Each endpoint will have its own parameters, so you’ll need to spend a little time with the Census API documentation to learn what you can use. In our case, we are requesting data for Census tracts in Minnesota. The built-in validate_state() function can be used here to convert 'MN' to an appropriate FIPS code.\n\nguess_dtypes and return_geoid are convenience parameters that you’ll want to use judiciously. guess_dtypes tries to guess which columns to convert to numeric, and return_geoid tries to find columns to concatenate into a GEOID column that can be used for merging to Census shapes. These arguments won’t be appropriate for every API endpoint.\n\nWith our data in hand, we can do an inner merge and map the result:\n\nimport matplotlib.pyplot as plt\n\ntc_tract_age = tc_tracts.merge(mn_median_age, how = \"inner\", on = \"GEOID\")\n\ntc_tract_age.plot(column = \"B01002_001E\", legend = True)\n\nplt.title(\"Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS\")\n\nText(0.5, 1.0, 'Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS')\n\n\n\n\n\n\n\nAnalyzing spatial clustering with PySAL\nThe PySAL family of Python packages is central to the work of anyone who needs to analyze spatial data in Python. The esda package makes the calculation of the local Moran’s I statistic remarkably smooth. We will generate a Queen’s case spatial weights object (see my book for more technical details) to represent relationships between Census tracts and their neighbors, then call the Moran_Local() function to calculate the LISA statistics.\n\nfrom libpysal import weights\nimport esda\n\ntc_clean = tc_tract_age.copy().dropna().filter(['GEOID', 'B01002_001E', 'geometry'])\n\nw = weights.contiguity.Queen.from_dataframe(tc_clean)\n\nm = esda.Moran_Local(tc_clean['B01002_001E'], w, seed = 1983)\n\ntype(m)\n\nesda.moran.Moran_Local\n\n\nWe get an object of type Moran_Local which stores our analytical result. The documentation provides a comprehensive overview of the object’s attributes. We’ll grab two here and add them back to our Census tract dataset as new columns: q, which is the LISA quadrant (denoting the cluster type); and p_sim, which gives us a pseudo-p-value based on conditional permutation (see here for further discussion of this concept).\nUsing this information, we can identify “significant” and “non-significant” clusters and generate some more informative labels.\n\nimport numpy as np\n\n# We can extract the LISA quadrant along with the p-value from the lisa object\ntc_clean['quadrant'] = m.q\ntc_clean['p_sim'] = m.p_sim\n# Convert all non-significant quadrants to zero\ntc_clean['quadrant'] = np.where(tc_clean['p_sim'] > 0.05, 0, tc_clean['quadrant'])\n\n# Get more informative descriptions\ntc_clean['quadrant'] = tc_clean['quadrant'].replace(\n  to_replace = {\n    0: \"Not significant\",\n    1: \"High-high\",\n    2: \"Low-high\",\n    3: \"Low-low\",\n    4: \"High-low\"\n  }\n)\n\ntc_clean.head()\n\n\n\n\n\n  \n    \n      \n      GEOID\n      B01002_001E\n      geometry\n      quadrant\n      p_sim\n    \n  \n  \n    \n      0\n      27053103700\n      29.3\n      POLYGON ((-93.25825 44.98358, -93.25790 44.983...\n      Not significant\n      0.070\n    \n    \n      1\n      27053104100\n      28.2\n      POLYGON ((-93.31847 44.98174, -93.31847 44.983...\n      Low-low\n      0.043\n    \n    \n      2\n      27053104400\n      32.4\n      POLYGON ((-93.28158 44.97790, -93.28153 44.978...\n      Not significant\n      0.228\n    \n    \n      3\n      27053105100\n      44.6\n      POLYGON ((-93.32873 44.96012, -93.32873 44.960...\n      Not significant\n      0.127\n    \n    \n      4\n      27053105400\n      33.2\n      POLYGON ((-93.26972 44.96807, -93.26926 44.969...\n      Not significant\n      0.310\n    \n  \n\n\n\n\n\n\nBuilding an interactive LISA map\nWe now have all the information necessary to map LISA clusters. I’m going to show a workflow that differs slightly from typical LISA maps like the one illustrated in my book. One disadvantage of static LISA maps is that they assume an analyst has familiarity with the region under study. Without this familiarity, it can be difficult to determine exactly which locations are represented by different cluster types.\nEnter the .explore() GeoDataFrame method in GeoPandas. .explore() is an interface to Leaflet.js through Folium. Simply calling .explore() on a GeoDataFrame gets you started interactively exploring your spatial data; however, the method itself is a fairly full-featured interactive mapping engine.\nWith a little customization, we can build out an informative interactive map showing our LISA analysis of median age by Census tract in the Twin Cities. Here’s how we do it:\n\nWe choose \"quadrant\" as the column to visualize, and pass a list of colors to cmap to align with the typical color scheme used for LISA mapping (with some small modifications to improve visibility).\nlegend = True adds an informative legend, and a muted grey basemap is selected with tiles.\nThe various _kwds parameters are quite powerful, as this is how you will do more fine-grained customization of your map. We’ll reduce the line weight of our polygons to 0.5, and importantly do some customization of the popup to change the column names to informative aliases. Click on a Census tract to see what you get!\n\n\n# Build a LISA cluster map \ntc_clean.explore(column = \"quadrant\", \n                 cmap = [\"red\", \"hotpink\", \"deepskyblue\", \"blue\", \"lightgrey\"], \n                 legend = True, \n                 tiles = \"CartoDB positron\", \n                 style_kwds = {\"weight\": 0.5}, \n                 legend_kwds = { \"caption\": \"LISA quadrant\"}, \n                 tooltip = False, \n                 popup = True,\n                 popup_kwds = {\n                    \"aliases\": [\"GEOID\", \"Median age\", \"LISA quadrant\", \"Pseudo p-value\"]\n                 })\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nOur analytical result shows that younger areas tend to be found nearer to the Minneapolis / St. Paul urban cores, and older areas cluster in the western, southern, and northeastern suburbs. Spatial outliers are scattered throughout the region, and the map’s interactivity allows us to zoom in and click to understand these outliers in greater detail.\nTry out this workflow for yourself, and follow along here for more of my favorite examples from Analyzing US Census Data translated to Python over the next few months."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Kyle Walker, a spatial data science researcher, educator, and consultant. Here is some information about my current work:\n\nI am the author of the book Analyzing US Census Data: Methods, Maps, and Models in R, which is free to read online and is forthcoming with CRC Press in 2022.\n\nI’m an R developer actively working on the following packages:\n\ntidycensus, which helps R users get demographic & spatial data from the US Census Bureau ready-to-go for use in their analyses;\ntigris, which downloads US Census Bureau spatial data and loads it directly into R as simple features objects;\nmapboxapi, an R interface to Mapbox web services. Use the package to optimize routes, draw isochrones, read and write vector tiles, use custom Mapbox maps in Leaflet projects, and more;\ncrsuggest, which gives R users projected coordinate system suggestions for their spatial datasets.\n\n\nI’m an academic researching data science and visualization tools for spatial demography. I’m currently teaching courses in exploratory data analysis with Python and introductory Urban Studies.\n\nI am Director of Research at a boutique data science and strategy firm, where we use cutting-edge geospatial & machine learning methods to improve companies’ business outcomes.\n\nI also consult through my personal firm, Walker Data, where I work with individual clients and organizations to integrate tools like tidycensus into their workflows and to learn R and spatial data analysis.\n\nIf you are interested in working with me, send me a note at kyle@walker-data.com and let’s discuss your idea!"
  },
  {
    "objectID": "posts/lodes-commutes/index.html",
    "href": "posts/lodes-commutes/index.html",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "",
    "text": "In Chapter 11 of my book Analyzing US Census Data, I explore a sampling of the variety of government datasets that are available for the United States. One of the most useful of these datasets is LODES (LEHD Origin-Destination Employment Statistics). LODES is a synthetic dataset that represents, down to the Census block level, job counts by workplace and residence as well as the flows between them.\nGiven that LODES data are tabulated at the Census block level, analysts will often want to merge the data to Census geographic data like what is accessible in the pygris package. pygris includes a function, get_lodes(), that is modeled after the excellent lehdr R package by Jamaal Green, Dillon Mahmoudi, and Liming Wang.\nThis post will illustrate how to analyze the origins of commuters to the Census tract containing Apple’s headquarters in Cupertino, CA. In doing so, I’ll highlight some of the data wrangling utilities in pandas that allow for the use of method chaining, and show how to merge data to pygris shapes for mapping. The corresponding section in Analyzing US Census Data to this post is “Analyzing labor markets with lehdr.”"
  },
  {
    "objectID": "posts/lodes-commutes/index.html#acquiring-and-wrangling-lodes-data",
    "href": "posts/lodes-commutes/index.html#acquiring-and-wrangling-lodes-data",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "Acquiring and wrangling LODES data",
    "text": "Acquiring and wrangling LODES data\nTo get started, let’s import the functions and modules we need and give get_lodes() a try. get_lodes() requires specifying a state (as state abbreviation) and year; we are getting data for California in 2019, the most recent year currently available. The argument lodes_type = \"od\" tells pygris to get origin-destination flows data, and cache = True will download the dataset (which is nearly 100MB) to a local cache directory for faster use in the future.\n\nfrom pygris import tracts \nfrom pygris.data import get_lodes\nimport matplotlib.pyplot as plt\n\nca_lodes_od = get_lodes(\n    state = \"CA\",\n    year = 2019,\n    lodes_type = \"od\",\n    cache = True\n)\n\nca_lodes_od.head()\n\n\n\n\n\n  \n    \n      \n      w_geocode\n      h_geocode\n      S000\n      SA01\n      SA02\n      SA03\n      SE01\n      SE02\n      SE03\n      SI01\n      SI02\n      SI03\n      createdate\n    \n  \n  \n    \n      0\n      060014001001007\n      060014001001044\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      20211018\n    \n    \n      1\n      060014001001007\n      060014001001060\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      20211018\n    \n    \n      2\n      060014001001007\n      060014038002002\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      20211018\n    \n    \n      3\n      060014001001007\n      060014041021003\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      20211018\n    \n    \n      4\n      060014001001007\n      060014042002012\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      20211018\n    \n  \n\n\n\n\nThe loaded dataset, which has nearly 16 million rows, represents synthetic origin-destination flows from Census block to Census block in California in 2019. Columns represent the Census block GEOIDs for both workplace and residence, as well as job counts for flows between them. S000 represents all jobs; see the LODES documentation for how other breakouts are defined.\n16 million rows is a lot of data to deal with all at once, so we’ll want to do some targeted data wrangling to make this more manageable. We’ll do so using a method chain, which is my preferred way to do data wrangling in Python given that I come from an R / tidyverse background. The code takes the full origin-destination dataset, rolls it up to the Census tract level, then returns (by Census tract) the number of commuters to Apple’s Census tract in Cupertino.\n\napple = (\n    ca_lodes_od\n    .assign(w_tract = ca_lodes_od['w_geocode'].str.slice(stop = 11),\n            h_tract = ca_lodes_od['h_geocode'].str.slice(stop = 11))\n    .query('w_tract == \"06085508102\"')\n    .groupby('h_tract', as_index = False)\n    .agg({'S000': sum})\n    .rename({'S000': 'apple_workers'}, axis = 1)\n)\n\napple.head()\n\n\n\n\n\n  \n    \n      \n      h_tract\n      apple_workers\n    \n  \n  \n    \n      0\n      06001400100\n      3\n    \n    \n      1\n      06001400200\n      3\n    \n    \n      2\n      06001400300\n      3\n    \n    \n      3\n      06001400400\n      2\n    \n    \n      4\n      06001400500\n      4\n    \n  \n\n\n\n\nLet’s step through how we did this:\n\nThe .assign() method is used to calculate two new Census tract columns. A great thing about Census GEOIDs is that child geographies (like Census blocks) contain information about parent geographies. In turn, we can calculate Census tract GEOIDs by slicing block GEOIDs for the first 11 characters.\n\n.query() is used to subset our data. We only want rows representing commuters to the Apple campus (or the area around it), so we query for that specific tract ID.\nNext, we’ll roll up our data to the tract level. We’ll first group the data by home Census tract with .groupby(), then calculate group sums with .agg().\n\nFinally, we use a dictionary passed to .rename() to give the jobs column a more interpretable name.\n\nNext, we’ll repeat this process to tabulate the total number of workers by home Census tract to be used as a denominator. After that, we can merge the Apple-area commuters dataset back in, and calculate a rate per 1000. Note the lambda notation used in the final step of the method chain: this allows us to refer to the dataset that is being created by the chain.\n\napple_commuters = (\n    ca_lodes_od\n    .assign(h_tract = ca_lodes_od['h_geocode'].str.slice(stop = 11))\n    .groupby('h_tract', as_index = False)\n    .agg({'S000': sum})\n    .rename({'S000': 'total_workers'}, axis = 1)\n    .merge(apple, on = 'h_tract')\n    .assign(apple_per_1000 = lambda x: 1000 * (x['apple_workers'] / x['total_workers']))\n)\n\napple_commuters.head()\n\n\n\n\n\n  \n    \n      \n      h_tract\n      total_workers\n      apple_workers\n      apple_per_1000\n    \n  \n  \n    \n      0\n      06001400100\n      1412\n      3\n      2.124646\n    \n    \n      1\n      06001400200\n      1095\n      3\n      2.739726\n    \n    \n      2\n      06001400300\n      2650\n      3\n      1.132075\n    \n    \n      3\n      06001400400\n      2341\n      2\n      0.854336\n    \n    \n      4\n      06001400500\n      2147\n      4\n      1.863065"
  },
  {
    "objectID": "posts/lodes-commutes/index.html#mapping-commute-flows-to-apple-headquarters",
    "href": "posts/lodes-commutes/index.html#mapping-commute-flows-to-apple-headquarters",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "Mapping commute flows to Apple headquarters",
    "text": "Mapping commute flows to Apple headquarters\nThe main purpose of the pygris package is to make the acquisition of US Census Bureau spatial data easy for Python users. Given that we have aggregated our data at the Census tract level, we can use the tracts() function to grab Census tract shapes for six counties in the San Francisco Bay Area. We’ll use the Cartographic Boundary shapefiles with cb = True to exclude most water area, and make sure to specify year = 2019 to match the 2019 LODES data.\n\nbay_tracts = tracts(state = \"CA\", cb = True,\n                    county = [\"San Francisco\", \"Alameda\", \"San Mateo\",\n                              \"Santa Clara\", \"Marin\", \"Contra Costa\"], \n                    year = 2019, cache = True)\n                    \nbay_tracts.plot()\n\nUsing FIPS code '06' for input 'CA'\n\n\nUsing FIPS code '075' for input 'San Francisco'\nUsing FIPS code '001' for input 'Alameda'\nUsing FIPS code '081' for input 'San Mateo'\nUsing FIPS code '085' for input 'Santa Clara'\nUsing FIPS code '041' for input 'Marin'\nUsing FIPS code '013' for input 'Contra Costa'\n\n\n<AxesSubplot:>\n\n\n\n\n\nWith our tracts in hand, we use the .merge() method to merge the tabulated LODES data to the Census tract shapes, then make a plot with geopandas’ plotting functionality.\n\napple_bay = bay_tracts.merge(apple_commuters, left_on = \"GEOID\", right_on = \"h_tract\")\n\napple_bay.plot(column = 'apple_per_1000', legend = True, \n               cmap = \"cividis\", figsize = (8, 8), \n               k = 7, scheme = \"naturalbreaks\",\n               legend_kwds = {\"loc\": \"lower left\"})\n\nplt.title(\"Apple-area commuters (rate per 1000 total commuters)\\n2019 LODES data, Bay Area Census tracts\", fontsize = 12)\n\nax = plt.gca()\n\nax.set_axis_off()\n\n\n\n\nCommuters to Apple’s tract tend to be concentrated around that tract; however, several neighborhoods in San Francisco proper send dozens of commuters per 1000 total commuters south to Cupertino."
  }
]