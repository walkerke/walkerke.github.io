[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "WALKER DATA",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nVisualizing accessibility surfaces in R\n\n\n\n\n\n\n\nr\n\n\ngis\n\n\ndata science\n\n\nspatial analysis\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nIterative ‘mapping’ in R\n\n\n\n\n\n\n\nr\n\n\ngis\n\n\ndata science\n\n\ncensus\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nTravel-time isochrones with Mapbox, Python, and GeoPandas\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\nnavigation\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nBuilding custom regions from 2020 Census blocks in Python\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nMapping jobs and commutes with 2020 LODES data and deck.gl\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nUsing your favorite Python packages in ArcGIS Pro\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nDistance and proximity analysis in Python\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\nnavigation\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing labor markets in Python with LODES data\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nKyle Walker\n\n\n\n\n\n\n  \n\n\n\n\nExploratory spatial data analysis with Python\n\n\n\n\n\n\n\npython\n\n\ngis\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nKyle Walker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WALKER DATA",
    "section": "",
    "text": "WALKER DATA is a data science consultancy specializing in geospatial and demographic data support for your business.\nPlease reach out to kyle@walker-data.com for consulting assistance with the following topics:\n\nDemographic analysis / Census data\nBusiness and location intelligence\nGeographic Information Systems (GIS) and spatial data science\nCustom data science workshops, including training in the R and Python programming languages\n\n\n\n\n\n\n\n  \n    \n      \n      Expert demographics, Census, and data science advisory\n    \n  \n  \n    \n      \n      Sophisticated location intelligence solutions for your business\n    \n  \n  \n    \n      \n      R and GIS training from the developer of the tidycensus R package\n    \n  \n  \n    \n      \n      Read the book Analyzing US Census Data: Methods, Maps, and Models in R\n    \n  \n\n\n\n\nTo receive on-going updates from Walker Data, consider signing up for the Walker Data mailing list:"
  },
  {
    "objectID": "posts/proximity-analysis/index.html",
    "href": "posts/proximity-analysis/index.html",
    "title": "Distance and proximity analysis in Python",
    "section": "",
    "text": "Spatial data science projects frequently require the calculation of proximity to resources. Analysts in fields like health care, real estate, retail, education, and more are commonly tasked with finding out what resources are near to a given location, and potentially develop strategies to fill identified gaps in resource access. In Chapter 7 of my book Analyzing US Census Data, I illustrate a workflow that shows how to analyze accessibility from Census tracts to major trauma hospitals in Iowa. In this post, I’ll show you how to reproduce the first part of that analysis in Python for the neighboring state of South Dakota.\nSimilar to the section in my book, this post will cover how to calculate proximity in two ways: using straight-line (Euclidean) distances in a projected coordinate system, and using driving times derived from a hosted navigation service. You’ll learn how to compute distances using built-in tools in geopandas, and also use the routingpy Python package to interact with Mapbox’s Navigation APIs."
  },
  {
    "objectID": "posts/proximity-analysis/index.html#getting-and-formatting-location-data",
    "href": "posts/proximity-analysis/index.html#getting-and-formatting-location-data",
    "title": "Distance and proximity analysis in Python",
    "section": "Getting and formatting location data",
    "text": "Getting and formatting location data\nTo get started, we’ll need to acquire and format data on both Census tracts and hospitals. We can get Census tract boundaries for South Dakota using my pygris package; we’ll then use the .to_crs() method to transform to an appropriate projected coordinate reference system for the area.\nThe hospitals can be downloaded from the US Department of Homeland Security’s Homeland Infrastructure Foundation-Level Data (HIFLD) portal. HIFLD includes open, frequently-updated datasets on “critical infrastructure” such as schools, pharmacies, and health care facilities. After reading in the downloaded file, we subset the data using a regular expression and .str.contains() to include only Level I and Level II trauma centers, and drop any duplicated records.\n\nimport geopandas as gp\nfrom pygris import tracts\n\n# Coordinate system: State Plane South Dakota North\nsd_tracts = tracts(\"SD\", cb = True, \n                   year = 2021, \n                   cache = True).to_crs(6571)\n\n\nhospitals = gp.read_file('Hospitals.geojson').to_crs(6571)\n\ntrauma = hospitals.loc[hospitals['TRAUMA'].str.contains(\"LEVEL I\\\\b|LEVEL II\\\\b|RTH|RTC\")]\n\ntrauma = trauma.drop_duplicates(['ID'])\n\nUsing FIPS code '46' for input 'SD'\n\n\nThe next step is to identify those trauma hospitals that are near to South Dakota. While the hospitals dataset does have a STATE column, we don’t want to filter the dataset for only those hospitals that are located in the state. In some cases, the nearest trauma hospital might be in a different state, and we don’t want to exclude those from our analysis.\nThe approach here identifies all trauma hospitals within 100 kilometers of the South Dakota border, or within the state itself. We use a sequence of common GIS operations with geopandas to accomplish this. Our steps include:\n\nCombining the South Dakota Census tracts into a single shape with the .dissolve() method;\nDrawing a new shape that extends to 100km beyond the South Dakota border with the .buffer() method;\nUsing an inner spatial join to retain only those trauma centers that fall within the 100km buffer shape.\n\n\nsd_buffer = gp.GeoDataFrame(geometry = sd_tracts.dissolve().buffer(100000))\n\nsd_trauma = trauma.sjoin(sd_buffer, how = \"inner\")\n\nAfter running this operation, we can draw a quick plot to show the relationships between Census tracts and hospitals as in Section 7.4.1 of Analyzing US Census Data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize = (8, 5))\n\nsd_tracts.plot(ax = ax, color = \"grey\")\nsd_trauma.plot(ax = ax, color = \"red\")\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nThere are seven Level I or Level II trauma centers within 100km of South Dakota (in fact, all are Level II). In fact, only three are actually in the state, with two in Sioux Falls and one in Rapid City. Three others are in North Dakota (Bismarck and Fargo) and one is just across the border in Sioux City, Iowa."
  },
  {
    "objectID": "posts/proximity-analysis/index.html#calculating-distances-to-trauma-centers",
    "href": "posts/proximity-analysis/index.html#calculating-distances-to-trauma-centers",
    "title": "Distance and proximity analysis in Python",
    "section": "Calculating distances to trauma centers",
    "text": "Calculating distances to trauma centers\nThe simplest way to calculate proximity is with straight-line distances. In a projected coordinate system, this amounts to little more than Euclidean geometry, and such distance calculations are readily available to us using the .distance() method in geopandas. Conceptually, we’ll want to think through how to represent polygon-to-point distances. The most accurate approach would likely be to find the population-weighted centroid of each Census tract using some underlying dataset like Census blocks. In the example here, I’m taking a more simplistic approach by finding the geographic centroid of each tract. The centroids are found in the centroid attribute of any polygon GeoDataFrame.\nOnce the centroids are identified, we can iterate over them with apply and build a dataset that represents a distance matrix between Census tracts and trauma hospitals. Distances are calculated in meters, the base unit of our coordinate reference system (State Plane South Dakota North).\n\ntract_centroids = sd_tracts.centroid\n\ndist = tract_centroids.geometry.apply(lambda g: sd_trauma.distance(g, align = False))\n\ndist.head()\n\n\n\n\n\n\n\n\n2321\n4488\n4489\n4500\n5453\n5454\n5455\n\n\n\n\n0\n179156.152185\n422441.507300\n422155.094660\n313038.398120\n490132.864570\n62861.191081\n61633.946651\n\n\n1\n315344.252959\n276978.438522\n276708.176024\n254769.314501\n384118.434400\n208119.760220\n206470.685056\n\n\n2\n275529.979201\n354138.644057\n353818.847981\n213424.554461\n495938.859489\n157302.191011\n156735.306982\n\n\n3\n187763.417249\n408402.306426\n408161.751845\n362094.468043\n419202.830653\n107982.166667\n105743.400331\n\n\n4\n655092.671279\n344550.256147\n344760.480163\n605274.028698\n87369.870363\n592598.357613\n590388.549939\n\n\n\n\n\n\n\nLet’s take a quick look at how distance to the nearest trauma center varies around South Dakota. We’ll use the .min() method to find the minimum distance to a hospital for each row, then divide the result by 1000 to convert the distances to kilometers. We can then draw a histogram to review the distribution.\n\nmin_dist = dist.min(axis = 'columns') / 1000\n\nsns.histplot(min_dist, binwidth = 10)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\nOver 60 Census tracts are within 10km of a trauma center, reflecting tracts located within the population centers of Rapid City and Sioux Falls. However, many tracts in the state are beyond 100km from the nearest trauma center, with 26 200km or more away.\nWe know that in rural areas, however, straight-line distances can be misleading. Given the geography of highway networks, accessibility to a trauma center is mediated through accessibility to that road network. Let’s take a look at an alternative approach to calculating proximity to hospitals with a hosted routing service."
  },
  {
    "objectID": "posts/proximity-analysis/index.html#finding-travel-times-to-trauma-centers",
    "href": "posts/proximity-analysis/index.html#finding-travel-times-to-trauma-centers",
    "title": "Distance and proximity analysis in Python",
    "section": "Finding travel-times to trauma centers",
    "text": "Finding travel-times to trauma centers\nAnalysts who need to calculate proximity along a road network in Python have multiple options available to them. One way is to build a network using a tool like OSMnx; another way, shown here, is to connect to a hosted navigation service. In Analyzing US Census Data, I demonstrate how to use my mapboxapi R package to calculate a travel-time matrix between Census tracts and hospitals. In Python, we can connect to Mapbox’s navigation services with the routingpy package, an interface to several hosted navigation APIs.\nIn routingpy, Mapbox’s web services are referenced as MapboxOSRM. We’ll need a Mapbox account and access token to access these services; I’m storing mine in a variable named mapbox_key. I then initalize a connection to Mapbox with MapboxOSRM(), which I am calling mb.\n\nfrom routingpy.routers import MapboxOSRM\n\nmb = MapboxOSRM(api_key = mapbox_key)\n\nIn the mapboxapi R package, the processing required to prepare datasets for the Mapbox APIs is taken care of internally. To calculate a travel-time matrix in Python, we’ll need to do some data processing to format our Census tracts and hospitals in the correct way before we send them to the routing service.\nBelow, I’m defining a function points_to_coords() to convert a given GeoDataFrame of points to a list of lists, with each list element representing the XY coordinates of a given input point in the dataset. We’ll then call the function on the Census tract centroids and the trauma hospitals datasets.\n\n# Function to convert points to coordinates\ndef points_to_coords(input):\n  geom = input.to_crs(4326).geometry\n\n  return [[g.x, g.y] for g in geom]\n\n# Generate list of coordinates\ntract_coords = points_to_coords(tract_centroids)\nhospital_coords = points_to_coords(sd_trauma)\n\nNext, we’ll set up some custom code that allows for the creation of a 242 by 7 travel-time matrix. This is necessary because Mapbox’s Matrix API only allows for a maximum of 25 coordinate pairs per request, and we have 249 total! Let’s run the code, then walk through the steps we took.\n\nimport pandas as pd\n\nsplit_size = 25 - len(hospital_coords)\n\nchunks = [tract_coords[x:x + split_size] for x in range(0, len(tract_coords), split_size)]\n\ntimes_list = []\n\nfor chunk in chunks:\n\n  all_coords = chunk + hospital_coords\n\n  # Find the indices of origin and destination\n  origin_ix = [x for x in range(0, len(chunk), 1)]\n  hospital_ix = [y for y in range(len(chunk), len(all_coords), 1)]\n\n  # Run the travel-time matrix\n  times = mb.matrix(locations = all_coords,\n                    profile = \"driving\",\n                    sources = origin_ix,\n                    destinations = hospital_ix)\n\n  # Convert to a dataframe\n  times_df = pd.DataFrame(times.durations)\n\n  times_list.append(times_df)\n\nall_times = pd.concat(times_list, ignore_index = True) \n\nall_times.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n7888.9\n22103.4\n22233.8\n11812.5\n19938.0\n3821.1\n3672.0\n\n\n1\n14748.0\n14480.9\n14611.3\n12718.5\n17358.1\n10649.7\n10531.1\n\n\n2\n10251.6\n18023.1\n18153.5\n7747.9\n22173.1\n6183.8\n6034.7\n\n\n3\n8646.7\n20428.2\n20558.6\n15953.4\n15610.0\n4548.4\n4429.8\n\n\n4\n26589.6\n17174.6\n17177.2\n27658.8\n3819.9\n22491.3\n22372.7\n\n\n\n\n\n\n\nYou can interpret the code above as follows:\n\nGiven that we have far more coordinates than the Matrix API’s limit of 25, we need to split up our origin coordinates into chunks. We identify a split_size as 25 minus the number of hospitals (7); this is 18. We then split the Census tract centroid coordinates into chunks of 18 using a list comprehension.\nNext, we initialize a list to store our chunked output, and iterate through the chunks. This process involves the following:\n\nWe’ll combine the list of origin coordinates (for a given chunk) and the hospital coordinates in the object all_coords;\nNext, we identify the indices of the origin coordinates and the hospital coordinates in all_coords. This is critical information that we’ll need to pass along to the Mapbox Matrix API.\n\nWe use mb.matrix() to make a request to the Matrix API. The request requires a list of coordinates; a travel profile; and the source and destination indices.\n\nOnce we get the results back, we’ll create a pandas DataFrame and append to times_list.\n\n\nFinally, we use pd.concat() to combine the chunked results into a single DataFrame, which you see above. Values represent the drive-time in seconds between Census tract centroids and the trauma hospitals.\n\nOur last steps before plotting involve finding the minimum travel-time from each Census tract to a trauma hospital, converting to minutes, then adding back to the Census tracts GeoDataFrame as a column named 'time'. Using the GeoDataFrame method .plot() with some customization, we can reproduce the plot in Analyzing US Census Data.\n\nmin_times = all_times.min(axis = \"columns\") / 60\n\nsd_tracts['time'] = min_times\n\nsd_tracts.plot(column = \"time\", legend = True, figsize = (8, 5),\n               cmap = \"magma\",\n               legend_kwds = {\"location\": \"top\",\n                              \"shrink\": 0.5})\n\nplt.title(\"Travel-time (minutes) to nearest\\nLevel I or Level II trauma hospital\\n\\n\\n\\n\")\n\nax = plt.gca()\n\nax.set_axis_off()\n\nax.annotate('Census tracts in South Dakota\\nData sources: US Census Bureau, US DHS, Mapbox', \n            xy=(0.1, 0.1), xycoords='figure fraction',\n            fontsize=8, ha='left', va='top')\n\nText(0.1, 0.1, 'Census tracts in South Dakota\\nData sources: US Census Bureau, US DHS, Mapbox')\n\n\n\n\n\nThe map shows distinct gaps in accessibility to Level II trauma centers across the state of South Dakota. While neighborhoods in Rapid City, Sioux Falls, and the southeastern corner of the state near Sioux City are a short drive from a trauma hospital, some central South Dakota Census tracts are more than three and a half hours away from the nearest Level II trauma hospital by car.\nIf you’ve found this post useful, consider picking up a copy of my book Analyzing US Census Data: Methods, Maps, and Models in R, which hits shelves next month. I’ll keep sharing Python spatial data science workflows on this blog as well, so keep an eye out for future posts!"
  },
  {
    "objectID": "posts/pygris-arcgis/index.html",
    "href": "posts/pygris-arcgis/index.html",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "",
    "text": "Last week, I gave a workshop on working with geographic data in Python with the University of Michigan’s Social Science Data Analysis Network. The workshop focused on pygris, my new Python package for working with US Census Bureau geographic data resources. I was asked by multiple attendees if pygris works within ArcGIS Pro Python notebooks. I did not know the answer at the time, but it seemed like pygris - and other geospatial data packages in Python - combined with ArcGIS Pro could be quite powerful. I tested this process out, and it turns out that pygris and ArcGIS Pro work quite well together! Read on for a tutorial for how to use pygris (and your other favorite Python packages) in ArcGIS Pro.\nThis post was tested on ArcGIS Pro version 3.1."
  },
  {
    "objectID": "posts/pygris-arcgis/index.html#setting-up-a-new-conda-environment",
    "href": "posts/pygris-arcgis/index.html#setting-up-a-new-conda-environment",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "Setting up a new conda environment",
    "text": "Setting up a new conda environment\nArcGIS Pro ships with conda, a popular package manager for scientific Python. The default conda environment used by ArcGIS Pro is called arcgispro-py3, which has many of the most popular Python libraries for scientific computing and all the Python libraries needed for ArcGIS Pro installed. While this handles many use-cases within ArcGIS Pro, you can’t install new packages into this environment outside of an approved list. This means that you wouldn’t be able to use newer packages like pygris without first creating a new environment.\nWe’ll want our new environment to include all of the packages already found in arcgispro-py3, so we’ll need to clone it. You’ll want to click the Start icon in Windows then click “All apps”. Navigate to “ArcGIS” and select the Python command prompt.\n\nFrom the command prompt, enter the following command:\nconda create --name YOUR_ENV_NAME --clone arcgispro-py3\nwhere YOUR_ENV_NAME is the chosen name of your new environment. I’m calling my environment pygris, as I want to use it with the pygris package. After the environment is set up, you can install packages into it using conda or pip. As pygris is not yet available on conda, we install it with the following command:\npip install pygris\nThis will install pygris into your new conda environment along with all required dependencies."
  },
  {
    "objectID": "posts/pygris-arcgis/index.html#using-your-new-environment-in-arcgis-pro",
    "href": "posts/pygris-arcgis/index.html#using-your-new-environment-in-arcgis-pro",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "Using your new environment in ArcGIS Pro",
    "text": "Using your new environment in ArcGIS Pro\nTo use your new environment in ArcGIS Pro, you’ll first need to activate it - but not from the command line. Launch ArcGIS Pro and click Settings &gt; Package Manager. In the upper-right corner of your screen, change the active environment to your new conda environment, e.g. “pygris”.\n\nLaunch a new project (mine is called pygris-blog), then launch a new ArcGIS Python Notebook by clicking Insert &gt; New Notebook. I’ve organized my panes from left to right to show Contents, Notebook, Map, then Catalog.\n\nYou should now be able to start using pygris and any other of your favorite Python packages (that are installed) in ArcGIS just as you would in a regular Jupyter notebook. I’m running the following code from last week’s workshop to grab 2017-2021 ACS data on the percent of the population age 25+ with a bachelor’s degree for New York City and erasing water area from Census tracts obtained with pygris. Note that I’m using a coordinate reference system in .to_crs() appropriate for New York City, “NAD83(2011) / New York Long Island” which has the EPSG code 6538. Your project may require a different CRS for your area.\nfrom pygris import tracts\nfrom pygris.utils import erase_water\nfrom pygris.data import get_census\n\nnyc_counties = [\"New York\", \"Bronx\", \"Richmond\", \n                \"Kings\", \"Queens\"]\n\nnyc_tracts = tracts(state = \"NY\", county = nyc_counties,\n                    year = 2021, cache = True).to_crs(6538)\n\nny_college = get_census(dataset = \"acs/acs5/profile\",\n                        variables = \"DP02_0068PE\",\n                        year = 2021,\n                        params = {\n                          \"for\": \"tract:*\",\n                          \"in\": \"state:36\"},\n                        guess_dtypes = True,\n                        return_geoid = True)\n                        \nnyc_merged = nyc_tracts.merge(ny_college, how = \"inner\", on = \"GEOID\")\n\nnyc_erase = erase_water(nyc_merged, area_threshold = 0.9)"
  },
  {
    "objectID": "posts/pygris-arcgis/index.html#mapping-python-generated-data-with-arcgis",
    "href": "posts/pygris-arcgis/index.html#mapping-python-generated-data-with-arcgis",
    "title": "Using your favorite Python packages in ArcGIS Pro",
    "section": "Mapping Python-generated data with ArcGIS",
    "text": "Mapping Python-generated data with ArcGIS\nIn a typical Python workflow, we might map our data with the .plot() method. We can do this directly in the notebook within ArcGIS Pro (note that we need %matplotlib inline and to use plt.show() to get this to work in an ArcGIS notebook). A major reason for linking pygris with ArcGIS Pro, however, is to gain access to ArcGIS Pro’s rich cartographic functionality. This will require a couple extra steps.\nWe’ll need to use the GeoAccessor class, which lives inside the ArcGIS API for Python. GeoAccessor allows us to interact with Pandas DataFrames and GeoPandas GeoDataFrames like our New York City object. Note that this was buggy prior to ArcGIS for Python version 2.0.1, so you’ll need to update the package if you have an older version.\nfrom arcgis.features import GeoAccessor\n\nnyc_arcgis = GeoAccessor.from_geodataframe(nyc_proj, column_name = \"geometry\")\nThe object nyc_arcgis is now a “spatially enabled data frame” that can interact with the ArcGIS suite of projects. ArcGIS Online users can publish the dataset as a feature layer with the to_featurelayer() method. Since we are using ArcGIS Pro, we will want to use to_featureclass() and write the layer to our project’s geodatabase. This process is very smooth; you would replace out_path with the path to your geodatabase.\nout_path = r\"C:\\Users\\kylewalker\\Documents\\ArcGIS\\Projects\\pygris-blog\\pygris-blog.gdb\\nyc_tract_education\"\n\nnyc_arcgis.spatial.to_featureclass(out_path)\nIn the Catalog pane, navigate to your project geodatabase. The new feature class (I’ve called mine nyc_tract_education should now be written there. Drag and drop it onto your map, and start exploring and mapping with ArcGIS’s cartography tools!"
  },
  {
    "objectID": "posts/python-isochrones/index.html",
    "href": "posts/python-isochrones/index.html",
    "title": "Travel-time isochrones with Mapbox, Python, and GeoPandas",
    "section": "",
    "text": "Travel-time isochrones are powerful analytical tools that represent the reachable area from a location for a given time and travel mode. In R, my package mapboxapi seamlessly integrates with R’s GIS infrastructure to allow for the use of Mapbox’s isochrones in spatial analysis workflows. In Python, there isn’t a package that directly connects Mapbox’s navigation toolkit to GeoPandas for spatial data analysis. However, these services are accessible via the routingpy Python package.\nIn this blog post, I’ll present a workflow to help you connect GeoPandas with Mapbox’s isochrone services via routingpy. You’ll be able to use GeoPandas POINT geometries as inputs and get back isochrone polygons as GeoDataFrames. The goal is to replicate some of the functionality of R’s mb_isochrone() function in Python.\nTo get started, let’s import a few libraries we’ll need, and make a connection to Mapbox’s navigation services which are named MapboxOSRM in routingpy. You’ll need a Mapbox account and a Mapbox access token to get this to work; you get 100,000 isochrones for free each month, so you shouldn’t have to worry about getting charged.\n\nimport geopandas as gp\nimport pandas as pd\nfrom shapely.geometry import Polygon\nfrom routingpy.routers import MapboxOSRM\nimport numpy as np\n\nmb = MapboxOSRM(api_key = \"YOUR KEY GOES HERE\")\n\nLet’s use a public libraries dataset in the city of Dallas, Texas as an example. Mapbox’s routing services will run for any location in the world covered by OpenStreetMap, so you can try this out for other datasets of interest as well.\n\ndallas_libraries = gp.read_file(\"https://egis.dallascityhall.com/resources/Downloads/ShpZip/Library/Libraries.zip\")\n\ndallas_libraries.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nNext comes the mb_isochrone() function. I’ve written this to work in a similar way to mb_ischrone() in R, though it is much more limited. Read through the comments in the code to get a sense of how it works.\n\ndef mb_isochrone(gdf, time = [5, 10, 15], profile = \"driving\"):\n\n    # Grab X and Y values in 4326\n    gdf['LON_VALUE'] = gdf.to_crs(4326).geometry.x\n    gdf['LAT_VALUE'] = gdf.to_crs(4326).geometry.y\n\n    coordinates = gdf[['LON_VALUE', 'LAT_VALUE']].values.tolist()\n\n    # Build a list of shapes\n    isochrone_shapes = []\n\n    if type(time) is not list:\n        time = [time]\n\n    # Use minutes as input, but the API requires seconds\n    time_seconds = [60 * x for x in time]\n\n    # Given the way that routingpy works, we need to iterate through the list of \n    # coordinate pairs, then iterate through the object returned and extract the \n    # isochrone geometries.  \n    for c in coordinates:\n        iso_request = mb.isochrones(locations = c, profile = profile,\n                                    intervals = time_seconds, polygons = \"true\")\n\n        for i in iso_request:\n            iso_geom = Polygon(i.geometry[0])\n            isochrone_shapes.append(iso_geom)\n\n    # Here, we re-build the dataset but with isochrone geometries\n    df_values = gdf.drop(columns = ['geometry', 'LON_VALUE', 'LAT_VALUE'])\n\n    time_col = time * len(df_values)\n\n    # We'll need to repeat the dataframe to account for multiple time intervals\n    df_values_rep = pd.DataFrame(np.repeat(df_values.values, len(time_seconds), axis = 0))\n    df_values_rep.columns = df_values.columns\n\n    isochrone_gdf = gp.GeoDataFrame(\n        data = df_values_rep,\n        geometry = isochrone_shapes,\n        crs = 4326\n    )\n\n    isochrone_gdf['time'] = time_col\n\n    # We are sorting the dataframe in descending order of time to improve visualization\n    # (the smallest isochrones should go on top, which means they are plotted last)\n    isochrone_gdf = isochrone_gdf.sort_values('time', ascending = False)\n\n    return(isochrone_gdf)\n\nLet’s try it out! The function runs seamlessly over a dataset of 29 input points, returning 29 5-minute isochrones that we can visualize with .explore().\n\nlibrary_isos = mb_isochrone(dallas_libraries, time = 5, \n                            profile = \"driving-traffic\")\n\nlibrary_isos.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nOne feature the R version of mb_isochrone() includes is the ability to get isochrones directly from an address. Here, we’ll need to geocode the address first and pass it to mb_isochrone(). Our result is an interactive map of multiple travel-times around our input address.\n\ndickies = gp.tools.geocode(\"1911 Montgomery St, Fort Worth, TX 76107\")\n\ndickies_isos = mb_isochrone(dickies, time = [5, 10, 15], profile = \"driving-traffic\")\n\ndickies_isos.explore(column = \"time\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nA great benefit of using isochrones with GeoPandas is that the isochrone shapes can be integrated into all sorts of spatial analysis workflows. If you are interested in integrating isochrones into your application or business workflow, or if you’d like a custom workshop to help you get up and running with these tools, please don’t hesistate to reach out to kyle@walker-data.com!"
  },
  {
    "objectID": "posts/lodes-commutes/index.html",
    "href": "posts/lodes-commutes/index.html",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "",
    "text": "In Chapter 11 of my book Analyzing US Census Data, I explore a sampling of the variety of government datasets that are available for the United States. One of the most useful of these datasets is LODES (LEHD Origin-Destination Employment Statistics). LODES is a synthetic dataset that represents, down to the Census block level, job counts by workplace and residence as well as the flows between them.\nGiven that LODES data are tabulated at the Census block level, analysts will often want to merge the data to Census geographic data like what is accessible in the pygris package. pygris includes a function, get_lodes(), that is modeled after the excellent lehdr R package by Jamaal Green, Dillon Mahmoudi, and Liming Wang.\nThis post will illustrate how to analyze the origins of commuters to the Census tract containing Apple’s headquarters in Cupertino, CA. In doing so, I’ll highlight some of the data wrangling utilities in pandas that allow for the use of method chaining, and show how to merge data to pygris shapes for mapping. The corresponding section in Analyzing US Census Data to this post is “Analyzing labor markets with lehdr.”"
  },
  {
    "objectID": "posts/lodes-commutes/index.html#acquiring-and-wrangling-lodes-data",
    "href": "posts/lodes-commutes/index.html#acquiring-and-wrangling-lodes-data",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "Acquiring and wrangling LODES data",
    "text": "Acquiring and wrangling LODES data\nTo get started, let’s import the functions and modules we need and give get_lodes() a try. get_lodes() requires specifying a state (as state abbreviation) and year; we are getting data for California in 2019, the most recent year currently available. The argument lodes_type = \"od\" tells pygris to get origin-destination flows data, and cache = True will download the dataset (which is nearly 100MB) to a local cache directory for faster use in the future.\n\nfrom pygris import tracts \nfrom pygris.data import get_lodes\nimport matplotlib.pyplot as plt\n\nca_lodes_od = get_lodes(\n    state = \"CA\",\n    year = 2019,\n    lodes_type = \"od\",\n    cache = True\n)\n\nca_lodes_od.head()\n\n\n\n\n\n\n\n\nw_geocode\nh_geocode\nS000\nSA01\nSA02\nSA03\nSE01\nSE02\nSE03\nSI01\nSI02\nSI03\ncreatedate\n\n\n\n\n0\n060014001001007\n060014001001044\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n20211018\n\n\n1\n060014001001007\n060014001001060\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n20211018\n\n\n2\n060014001001007\n060014038002002\n1\n1\n0\n0\n1\n0\n0\n0\n0\n1\n20211018\n\n\n3\n060014001001007\n060014041021003\n1\n0\n1\n0\n0\n0\n1\n0\n0\n1\n20211018\n\n\n4\n060014001001007\n060014042002012\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n20211018\n\n\n\n\n\n\n\nThe loaded dataset, which has nearly 16 million rows, represents synthetic origin-destination flows from Census block to Census block in California in 2019. Columns represent the Census block GEOIDs for both workplace and residence, as well as job counts for flows between them. S000 represents all jobs; see the LODES documentation for how other breakouts are defined.\n16 million rows is a lot of data to deal with all at once, so we’ll want to do some targeted data wrangling to make this more manageable. We’ll do so using a method chain, which is my preferred way to do data wrangling in Python given that I come from an R / tidyverse background. The code takes the full origin-destination dataset, rolls it up to the Census tract level, then returns (by Census tract) the number of commuters to Apple’s Census tract in Cupertino.\n\napple = (\n    ca_lodes_od\n    .assign(w_tract = ca_lodes_od['w_geocode'].str.slice(stop = 11),\n            h_tract = ca_lodes_od['h_geocode'].str.slice(stop = 11))\n    .query('w_tract == \"06085508102\"')\n    .groupby('h_tract', as_index = False)\n    .agg({'S000': sum})\n    .rename({'S000': 'apple_workers'}, axis = 1)\n)\n\napple.head()\n\n\n\n\n\n\n\n\nh_tract\napple_workers\n\n\n\n\n0\n06001400100\n3\n\n\n1\n06001400200\n3\n\n\n2\n06001400300\n3\n\n\n3\n06001400400\n2\n\n\n4\n06001400500\n4\n\n\n\n\n\n\n\nLet’s step through how we did this:\n\nThe .assign() method is used to calculate two new Census tract columns. A great thing about Census GEOIDs is that child geographies (like Census blocks) contain information about parent geographies. In turn, we can calculate Census tract GEOIDs by slicing block GEOIDs for the first 11 characters.\n\n.query() is used to subset our data. We only want rows representing commuters to the Apple campus (or the area around it), so we query for that specific tract ID.\nNext, we’ll roll up our data to the tract level. We’ll first group the data by home Census tract with .groupby(), then calculate group sums with .agg().\n\nFinally, we use a dictionary passed to .rename() to give the jobs column a more interpretable name.\n\nNext, we’ll repeat this process to tabulate the total number of workers by home Census tract to be used as a denominator. After that, we can merge the Apple-area commuters dataset back in, and calculate a rate per 1000. Note the lambda notation used in the final step of the method chain: this allows us to refer to the dataset that is being created by the chain.\n\napple_commuters = (\n    ca_lodes_od\n    .assign(h_tract = ca_lodes_od['h_geocode'].str.slice(stop = 11))\n    .groupby('h_tract', as_index = False)\n    .agg({'S000': sum})\n    .rename({'S000': 'total_workers'}, axis = 1)\n    .merge(apple, on = 'h_tract')\n    .assign(apple_per_1000 = lambda x: 1000 * (x['apple_workers'] / x['total_workers']))\n)\n\napple_commuters.head()\n\n\n\n\n\n\n\n\nh_tract\ntotal_workers\napple_workers\napple_per_1000\n\n\n\n\n0\n06001400100\n1412\n3\n2.124646\n\n\n1\n06001400200\n1095\n3\n2.739726\n\n\n2\n06001400300\n2650\n3\n1.132075\n\n\n3\n06001400400\n2341\n2\n0.854336\n\n\n4\n06001400500\n2147\n4\n1.863065"
  },
  {
    "objectID": "posts/lodes-commutes/index.html#mapping-commute-flows-to-apple-headquarters",
    "href": "posts/lodes-commutes/index.html#mapping-commute-flows-to-apple-headquarters",
    "title": "Analyzing labor markets in Python with LODES data",
    "section": "Mapping commute flows to Apple headquarters",
    "text": "Mapping commute flows to Apple headquarters\nThe main purpose of the pygris package is to make the acquisition of US Census Bureau spatial data easy for Python users. Given that we have aggregated our data at the Census tract level, we can use the tracts() function to grab Census tract shapes for six counties in the San Francisco Bay Area. We’ll use the Cartographic Boundary shapefiles with cb = True to exclude most water area, and make sure to specify year = 2019 to match the 2019 LODES data.\n\nbay_tracts = tracts(state = \"CA\", cb = True,\n                    county = [\"San Francisco\", \"Alameda\", \"San Mateo\",\n                              \"Santa Clara\", \"Marin\", \"Contra Costa\"], \n                    year = 2019, cache = True)\n                    \nbay_tracts.plot()\n\nUsing FIPS code '06' for input 'CA'\n\n\nUsing FIPS code '075' for input 'San Francisco'\nUsing FIPS code '001' for input 'Alameda'\nUsing FIPS code '081' for input 'San Mateo'\nUsing FIPS code '085' for input 'Santa Clara'\nUsing FIPS code '041' for input 'Marin'\nUsing FIPS code '013' for input 'Contra Costa'\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nWith our tracts in hand, we use the .merge() method to merge the tabulated LODES data to the Census tract shapes, then make a map with geopandas’ plotting functionality.\n\napple_bay = bay_tracts.merge(apple_commuters, left_on = \"GEOID\", right_on = \"h_tract\",\n                             how = \"left\")\n\napple_bay.fillna(0, inplace = True)\n\napple_bay.plot(column = 'apple_per_1000', legend = True, \n               cmap = \"cividis\", figsize = (8, 8), \n               k = 7, scheme = \"naturalbreaks\",\n               legend_kwds = {\"loc\": \"lower left\"})\n\nplt.title(\"Apple-area commuters (rate per 1000 total commuters)\\n2019 LODES data, Bay Area Census tracts\", fontsize = 12)\n\nax = plt.gca()\n\nax.set_axis_off()\n\n\n\n\nCommuters to Apple’s tract tend to be concentrated around that tract; however, several neighborhoods in San Francisco proper send dozens of commuters per 1000 total commuters south to Cupertino.\nThis is where the LODES section of my book chapter ends; however, a static map like this can be difficult to interpret for those less familiar with the Bay Area. The geopandas .explore() method can make this map interactive for exploration without much more code. We’ll also use the built-in Census geocoding interface in pygris to add a marker where Apple Park’s visitors center is located.\n\nfrom pygris.geocode import geocode\n\napple_bay_sub = apple_bay.filter(['GEOID', 'total_workers',\n                                  'apple_workers', 'apple_per_1000',\n                                  'geometry'])\n\nvisitor_center = geocode(\"10600 N Tantau Ave, Cupertino, CA 95014\",\n                         as_gdf = True)\n\nm = apple_bay_sub.explore(column = \"apple_per_1000\", cmap = \"cividis\",\n                          k = 7, scheme = \"naturalbreaks\", popup = True, \n                          tooltip = False,                   \n                          tiles = \"CartoDB positron\", \n                          style_kwds = {\"weight\": 0.5},\n                          legend_kwds = {\"caption\": \"Apple-area commuters per 1000\",\n                                          \"colorbar\": False},\n                          popup_kwds = {\"aliases\": ['Census tract', 'Total workers',\n                                                    'Apple-area commuters', 'Rate per 1000']})\n\nvisitor_center.explore(m = m, marker_type = \"marker\", tooltip = False)      \n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nIf you’ve found this post useful, follow along on Twitter, LinkedIn, or subscribe to my newsletter for more examples of how to translate topics from my book to Python in advance of the book’s release next month!"
  },
  {
    "objectID": "posts/census-regions/index.html",
    "href": "posts/census-regions/index.html",
    "title": "Building custom regions from 2020 Census blocks in Python",
    "section": "",
    "text": "Earlier this month, I gave a two-part workshop series on analyzing the newly-released 2020 Decennial US Census Data with R. If you missed out on the workshop series, you can buy the videos and materials on my Workshops page. One topic I addressed was how to handle the impact of differential privacy on block-level accuracy in the new Census Data.\nDifferential privacy refers to a method used by the Census Bureau to infuse “noise” into data products to preserve respondent confidentiality. Counts for larger areas and larger groups will still be accurate, but differential privacy makes smaller counts less reliable. In fact, the Census Bureau makes the following recommendation about block-level data:\n\nDON’T use data for individual blocks. Instead, aggregate data into larger areas, or use statistical models that combine data from many blocks. Block data are published to permit the analysis of user-constructed geographic areas composed of multiple blocks, for example, new voting districts that consist of collections of blocks within a politically defined geography.\n\nThis isn’t likely to be satisfactory advice for analysts for a couple key reasons. First, analysts working with Census data in rural areas often need block-level data to understand demographic trends, as block groups (the next level up in the Census hierarchy) may be too large in sparsely-populated areas. Second, “aggregating data” is not as simple as it sounds in the quote. Creating data aggregations requires an understanding of techniques in GIS and data science that may be beyond the knowledge of the average Census data user.\nIn this post, I’ll illustrate a technique for creating custom regions from Census block data. We’ll be using the pygeoda package for this task, a Python wrapper of the C++ library that powers GeoDa, a GUI tool for exploratory spatial data analysis and spatial modeling. Working with GeoDa in this way is particularly fun for me. I was a qualitative geographer in graduate school before encountering GeoDa. GeoDa was the tool that sparked an interested in spatial data science for me and in many ways motivated my eventual career path.\nLet’s grab some block data using pygris for Delta County, Texas, a rural county of about 5,000 residents northeast of the Dallas-Fort Worth metro area. If you haven’t previously cached the Texas block shapefile, this will take a few minutes to download.\n\nimport geopandas as gp\nimport pygeoda\nfrom pygris import blocks, block_groups\nfrom pygris.data import get_census\n\n# Get the block data for a county in Texas\ndelta_blocks = blocks(state = \"TX\", county = \"Delta\", year = 2020, cache = True)\n\nUsing FIPS code '48' for input 'TX'\n\n\nUsing FIPS code '119' for input 'Delta'\n\n\nGiven that Delta County is fairly small, we can use .explore() to make a performant interactive map of the 571 Census blocks.\n\ndelta_blocks.explore(tooltip = False, popup = True)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThere is one town of reasonable size in Delta County, Cooper. However, Census geography above the block level makes any sort of demographic analysis tricky. For example, we can briefly review block groups in Delta County:\n\ndelta_bgs = block_groups(state = \"TX\", county = \"Delta\", year = 2020, cache = True)\n\ndelta_bgs.explore(tooltip = False, popup = True)\n\nUsing FIPS code '48' for input 'TX'\n\n\nUsing FIPS code '119' for input 'Delta'\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe see that the 571 blocks in Delta County are organized into only 4 block groups. Cooper is bisected by two block groups, both of which include area outside the built-up area of the town. This means that we can’t really use block groups to do more detailed analysis of Cooper’s in-town demographics, and any other settlements in the county (like Pecan Gap in the northwest) are subsumed by much larger block groups.\nThe solution we’ll use is regionalization. Regionalization is the process of building larger, aggregated areas from smaller geographies in ways that are spatially coherent and account for the characteristics of those small areas. To get started, let’s grab some demographic data from the new Demographic and Housing Characteristics file. While there are many ways to get Census data in Python, pygris has a lower-level function, get_census(), to help you grab data from the Census API to merge to your Census shapes.\nWe’ll get data on total population and the non-Hispanic white population from the DHC at the block level, requesting for Delta County.\n\ndelta_data = get_census(\n    dataset = \"dec/dhc\",\n    year = 2020,\n    variables = [\"P1_001N\", \"P5_003N\"],\n    params = {\n        \"for\": \"block:*\",\n        \"in\": [\"state:48\", \"county:119\"]\n    },\n    return_geoid = True,\n    guess_dtypes = True\n)\n\nWe can then merge our block-level Census data to our block geometries and calculate some derived columns. As the block shapes acquired with blocks() have a column on land area, ALAND20, we can calculate population density; we’ll also calculate the percentage of the block population that is non-Hispanic white. These columns will be used in the regionalization algorithm to cluster together demographically similar blocks.\n\ndelta_geo = delta_blocks[['GEOID20', 'geometry', 'ALAND20']].merge(delta_data, left_on = \"GEOID20\",\n                                                                     right_on = \"GEOID\")\n\ndelta_geo[\"pop_density\"] = delta_geo[\"P1_001N\"] / delta_geo[\"ALAND20\"]\n\ndelta_geo[\"percent_white\"] = delta_geo[\"P5_003N\"] / delta_geo[\"P1_001N\"]\n\ndelta_geo.fillna(0, inplace = True)\n\ndelta_geo.head()\n\n\n\n\n\n\n\n\nGEOID20\ngeometry\nALAND20\nP1_001N\nP5_003N\nGEOID\npop_density\npercent_white\n\n\n\n\n0\n481199502001069\nPOLYGON ((-95.69040 33.37604, -95.68957 33.376...\n5896\n15\n9\n481199502001069\n0.002544\n0.600000\n\n\n1\n481199502001083\nPOLYGON ((-95.69610 33.36993, -95.69488 33.369...\n12686\n8\n7\n481199502001083\n0.000631\n0.875000\n\n\n2\n481199502002056\nPOLYGON ((-95.68872 33.37532, -95.68783 33.375...\n6681\n5\n3\n481199502002056\n0.000748\n0.600000\n\n\n3\n481199502001022\nPOLYGON ((-95.70546 33.37336, -95.70434 33.374...\n84159\n4\n1\n481199502001022\n0.000048\n0.250000\n\n\n4\n481199502002066\nPOLYGON ((-95.68616 33.37308, -95.68539 33.373...\n6291\n7\n6\n481199502002066\n0.001113\n0.857143\n\n\n\n\n\n\n\nAfter preparing our Census data, we can now move to spatial analysis. pygeoda.open() convers a GeoPandas GeoDataFrame to an object suitable for use with pygeoda. Next, we’ll create spatial weights to represent spatial relationships between Census blocks. We’ll use rook weights, which means that blocks are considered to be neighbors if they share at least one line segment between them. This step is critical for regionalization as we want to ensure that our regions are spatially coherent.\n\ndelta_gda = pygeoda.open(delta_geo)\n\nw = pygeoda.rook_weights(delta_gda)\n\nw\n\nWeights Meta-data:\n number of observations:                  571\n           is symmetric:                 True\n               sparsity: 0.008244361905404535\n        # min neighbors:                    1\n        # max neighbors:                   22\n       # mean neighbors:   4.7075306479859895\n     # median neighbors:                  4.0\n           has isolates:                False\n\n\nWe get some basic information about the weights object. The least-connected block has 1 neighbor, and the most-connected block has 22; the median number of neighbors is 4.\nWith this information in hand, we can run the regionalization algorithm. The algorithm we’ll choose is Max-p. Max-P regionalization attempts to find the maximum number of clusters that are spatially contiguous and exceed a given size threshold while maximizing within-cluster homogeneity. We’ll use population density and percent non-Hispanic white as our clustering variables, and total population as our bounding variable. Setting min_bound to 100 tells the algorithm that each derived region must have at least 100 people in it. A couple other technical details: max-p is highly sensitive to the algorithm’s starting point, so it is recommended to set a random-number seed (and potentially evaluate results among multiple seeds). For more stable performance, cpu_threads = 1 should also be used.\n\ncluster_variables = delta_gda[['pop_density', 'percent_white']]\n\nbound_variable = delta_gda['P1_001N']\n\nregions = pygeoda.maxp_greedy(\n    w = w,\n    data = cluster_variables,\n    method = \"fullorder-averagelinkage\",\n    bound_variable = bound_variable,\n    min_bound = 100,\n    random_seed = 1983,\n    cpu_threads = 1\n)\n\nThanks to the C++ back-end of pygeoda, the function is lightning-fast. The function returns a dict object with information about the regionalization solution; we’ll pluck the clusters out from it and assign it to our original GeoDataFrame as a column. We’ll then use a dissolve() operation to build new geographies from our regionalization solution, and calculate some derived statistics for those regions.\n\ndelta_geo['region'] = regions.get('Clusters')\n\ndelta_regions = delta_geo.dissolve(by = 'region', aggfunc = 'sum').reset_index()\n\ndelta_regions[\"pop_density\"] = delta_regions[\"P1_001N\"] / delta_regions[\"ALAND20\"]\n\ndelta_regions[\"percent_white\"] = delta_regions[\"P5_003N\"] / delta_regions[\"P1_001N\"]\n\nprint(f'Number of regions: {delta_regions.shape[0]}\\nMinimum population: {delta_regions[\"P1_001N\"].min()}')\n\nNumber of regions: 39\nMinimum population: 101\n\n\nWe see that the algorithm built 39 regions in Delta County. The minimum population of any region is 101, which indicates that our minimum population threshold specification was satisfied. Let’s take a look at the derived regions:\n\ndelta_regions.explore(column = \"region\", categorical = True, legend = False)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nZooming into Cooper shows that we have several new geographies built from Census blocks in the town, allowing us to do more detailed demographic analysis. Additionally, Pecan Gap gets its own aggregated geography in this solution. We can also use these regions in future data work, as block IDs are mapped to regions in the delta_geo object we created.\nRegionalization is a powerful tool in spatial data science, and it can be used to solve problems in a wide range of fields. If you want to learn more, check out my workshops or send me a note to discuss further!"
  },
  {
    "objectID": "posts/lodes-2020/index.html",
    "href": "posts/lodes-2020/index.html",
    "title": "Mapping jobs and commutes with 2020 LODES data and deck.gl",
    "section": "",
    "text": "Last month, version 8 of the LEHD Origin-Destination Employment Statistics (LODES) dataset was released. This long-awaited release includes data on workplaces, residences, and origin-destination flows for workers in 2020, along with a time series of these statistics back to 2002 enumerated at 2020 Census blocks.\nThe latest release of the pygris package for Python enables programmatic access to these new data resources with its get_lodes() function. This new release also allows you to request Census geometry or longitude / latitude coordinates along with your LODES data, making data visualization and mapping straightforward. Let’s try it out!"
  },
  {
    "objectID": "posts/lodes-2020/index.html#mapping-job-locations-by-census-block",
    "href": "posts/lodes-2020/index.html#mapping-job-locations-by-census-block",
    "title": "Mapping jobs and commutes with 2020 LODES data and deck.gl",
    "section": "Mapping job locations by Census block",
    "text": "Mapping job locations by Census block\nTo get started, let’s take care of some imports. We’ll be using the following:\n\nThe get_lodes() function in the pygris package gives us access to the brand-new LODES data. There is a lot more you can do with get_lodes(); review the package documentation for more examples.\n\npydeck is a Python interface to deck.gl, one of the most stunning data visualization libraries around. As you’ll see, deck.gl can help you create performant three-dimensional visualizations with large datasets.\n\nWe’ll also use matplotlib to do some custom color work for our maps.\n\nfrom pygris.data import get_lodes\nimport pydeck\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nThe first example will visualize the distribution of accommodation and food service workers by Census block in Kentucky. We can get this information from the LODES Worker Area Characteristics (WAC) dataset, which helps us understand the geography of jobs for small areas.\nThe latest version of pygris (0.1.5) includes some mapping helpers in get_lodes(). The new return_geometry parameter identifies the appropriate TIGER/Line shapefile to merge to the requested LODES data and returns a GeoPandas GeoDataFrame with geometry. An alternative approach, which we will be using here, uses the new return_lonlat parameter. This gives us a Pandas DataFrame with columns representing the centroid of the location. This representation of geography works quite well with deck.gl.\nLet’s get WAC data for the state of Kentucky in 2020.\nky_lodes_wac = get_lodes(\n  state = \"KY\", \n  year = 2020, \n  lodes_type = \"wac\",\n  cache = True,\n  return_lonlat = True\n)\nThe returned data have a host of columns representing jobs by category within that block, along with two additional columns, w_lon and w_lat, which represent the longitude and latitude of each block centroid.\nOur next step is to write a color-generating function to add some context to our visualization. For cartographers coming to deck.gl from other mapping libraries, color formatting can be tricky. deck.gl expects RGBA colors with values ranging from 0 to 255; while many mapping libraries translate column values to colors for you, we’ll need to do this manually.\nThe function, column_to_rgba(), normalizes an input column and converts it to a column where every element is a list of format [R, G, B, A] for a given color map cmap. We’ll use this function to add a column to our dataset, 'color', that is based on values in the CNS18 column (representing accommodation and food service jobs) and uses the viridis color palette.\ndef column_to_rgba(column, cmap, alpha):\n    normalized = (column - column.min()) / (column.max() - column.min())\n    my_cmap = plt.get_cmap(cmap)\n    colors = normalized.apply(lambda x: [int(i * 255) for i in mcolors.to_rgba(my_cmap(x, alpha = alpha))])\n\n    return colors\n  \n  \nky_lodes_wac['color'] = column_to_rgba(ky_lodes_wac['CNS18'], \"viridis\", 0.6)\nThe longitude / latitude data will work well for a deck.gl ColumnLayer. A column layer is a three-dimensional visualization that renders each location as a column, with height and color optionally scaled to a given characteristic in the dataset. This is a nice alternative to a choropleth map of jobs by block, as block polygons can be very irregular.\nlayer = pydeck.Layer(\n  \"ColumnLayer\",\n  ky_lodes_wac,\n  get_position=[\"w_lon\", \"w_lat\"],\n  auto_highlight=True,\n  elevation_scale=20,\n  pickable=True,\n  get_elevation = \"CNS18\",\n  get_fill_color = \"color\",\n  elevation_range=[0, 1000],\n  extruded=True,\n  coverage=1\n)\n\n# Set the viewport location\nview_state = pydeck.ViewState(\n  longitude=-85.4095567,\n  latitude=37.2086276,\n  zoom=6,\n  min_zoom=5,\n  max_zoom=15,\n  pitch=40.5,\n  bearing=-27.36\n)\n\ntooltip = {\"html\": \"Number of accommodation / food service jobs: {CNS18}\"}\n\n# Render\nr = pydeck.Deck(\n  layers=[layer], \n  initial_view_state=view_state, \n  map_style = \"light\", \n  tooltip = tooltip\n)\n\nr.to_html(\"ky_service.html\")\nBrowse the map and look for interesting patterns. Note how seamlessly deck.gl visualizes all 30,000 block locations in the dataset!"
  },
  {
    "objectID": "posts/lodes-2020/index.html#mapping-origin-destination-flows",
    "href": "posts/lodes-2020/index.html#mapping-origin-destination-flows",
    "title": "Mapping jobs and commutes with 2020 LODES data and deck.gl",
    "section": "Mapping origin-destination flows",
    "text": "Mapping origin-destination flows\nThe return_lonlat feature in get_lodes() also works great for representing origin-destination flows. The origin-destination dataset in LODES, acquired with lodes_type = \"od\", returns block-to-block flows for all home-to-work combinations in a given state.\nGiven that block-to-block flows could quickly get visually overwhelming, we may want to aggregate our data to a parent geography. Let’s acquire origin-destination flows for the state of Texas, and aggregate to the Census tract level with the argument agg_level = \"tract\".\ntx_od = get_lodes(\n  state = \"TX\", \n  year = 2020, \n  lodes_type=\"od\",\n  agg_level = \"tract\",\n  cache = True, \n  return_lonlat = True\n)\nThe data we get back includes h_lon and h_lat columns representing the centroid of the home Census tract, and w_lon and w_lat columns for the centroid of the work Census tract.\nWe’ll visualize these flows with a deck.gl ArcLayer; incidentally, the PyDeck documentation uses LODES data to show how ArcLayers work.\nLet’s refine the data first to answer a specific question. I live in Fort Worth, Texas, and a major growth area for the city is AllianceTexas, a fast-developing industrial and commercial corridor. We’ll generate a new object, top_commutes, that identifies those Census tracts sending at least 25 commuters to the Census tract containing the southern part of the Alliance airport.\ntop_commutes = tx_od.query('w_geocode == \"48439113932\" & S000 &gt;= 25')\nFrom here, we can basically replicate the example from the PyDeck documentation, but apply it to commute flows to Alliance in Fort Worth.\nimport pydeck\n\nGREEN_RGB = [0, 255, 0, 200]\nRED_RGB = [240, 100, 0, 200]\n\narc_layer = pydeck.Layer(\n  \"ArcLayer\",\n  data=top_commutes,\n  get_width=\"S000 / 5\",\n  get_source_position=[\"h_lon\", \"h_lat\"],\n  get_target_position=[\"w_lon\", \"w_lat\"],\n  get_tilt=15,\n  get_source_color=RED_RGB,\n  get_target_color=GREEN_RGB,\n  pickable=True,\n  auto_highlight=True\n)\n\nview_state = pydeck.ViewState(\n  latitude=32.708664, \n  longitude=-97.360546, \n  bearing=45, \n  pitch=50, \n  zoom=8\n)\n\ntooltip = {\"html\": \"{S000} jobs &lt;br /&gt; Home of commuter in red; work location in green\"}\nr = pydeck.Deck(\n  arc_layer, \n  initial_view_state=view_state, \n  tooltip=tooltip, \n  map_style = \"road\"\n)\n\nr.to_html(\"alliance_commuters.html\")\nWe get a compelling origin-destination flow map showing the locations that sent the most commuters to AllianceTexas in 2020.\n\n\n\n\nWorking with LODES data can have massive benefits for your projects and your business. If you’d like to discuss how to integrate these insights into your work, please don’t hesitate to reach out!"
  },
  {
    "objectID": "posts/esda-with-python/index.html",
    "href": "posts/esda-with-python/index.html",
    "title": "Exploratory spatial data analysis with Python",
    "section": "",
    "text": "In early 2023, the print copy of my book Analyzing US Census Data: Methods, Maps, and Models in R will be available for purchase. The response to the free online version of the book has been fantastic thus far. One question I commonly get asked, however, is “will you re-produce this for Python? I’d love to use this work but I don’t use R.”\nI don’t have plans to replicate all of my R work in Python, but I did get the itch in the second half of 2022 to learn Python package development. The result is pygris, which is a port of the R tigris package but with some additional features.\nTo celebrate the publication of my book as well as the pygris package, I’m launching a blog series to illustrate how to reproduce some of my favorite examples from my book in Python. Each example will feature pygris. Follow along - I hope you find it useful!"
  },
  {
    "objectID": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "href": "posts/esda-with-python/index.html#mapping-local-morans-i-lisa-clusters-in-python",
    "title": "Exploratory spatial data analysis with Python",
    "section": "Mapping Local Moran’s I (LISA) clusters in Python",
    "text": "Mapping Local Moran’s I (LISA) clusters in Python\nCorresponding section in Analyzing US Census Data: Identifying clusters and spatial outliers with local indicators of spatial association (LISA)\nI came into graduate school intending to be a qualitative researcher, but I really got excited about spatial data analysis (and changed my career trajectory) when I learned GeoDa, a GUI tool for exploratory spatial data analysis. The method in GeoDa that resonated with me the most was the local form of the Moran’s I, an example of a LISA (local indicators of spatial association) statistic.\nLISAs are exploratory tools that help you make sense of spatial patterns in a dataset. They help surface preliminary answers to these questions:\n\nWhere are there concentrations of high attribute values in my spatial dataset?\nConversely, where can I find concentrations of low attribute values in my data?\nFinally, are there any unexpected values in my dataset, given the characteristics of their neighbors? These “spatial outliers” can be above-average values surrounded by low values, or below-average values surrounded by high values.\n\nThis post will walk you through how to create an interactive LISA map of median age by Census tract from the 2017-2021 American Community Survey, similar to the example in Section 7.7.3 of my book. That section of my book covers more technical details about LISA if you are interested in reading further.\n\nGetting the data with pygris\nTo get started, we’ll use pygris to get the data required for our analysis. The core functionality in pygris is a suite of functions to return US Census Bureau TIGER/Line shapefiles as GeoPandas GeoDataFrames.\nLet’s import the tracts() function to demonstrate how this works for the Minneapolis-St. Paul, Minnesota area. We’ll define a list of the seven core Twin Cities counties, and request Census tract boundaries for those counties with tracts(). pygris functions translate state names/abbreviations and county names internally to FIPS codes, so there is no need to look them up.\nThe argument year = 2021 gives back the 2021 version of the Census tract boundaries, which will be important as we’ll be matching to corresponding 2021 ACS data. Finally, the argument cache = True stores the downloaded shapefile in a local cache, which means that I won’t need to download it again from the Census website in future projects.\n\nfrom pygris import tracts\n\ntc_counties = [\"Hennepin\", \"Ramsey\", \"Scott\", \n               \"Carver\", \"Dakota\", \"Washington\", \"Anoka\"]\n\ntc_tracts = tracts(state = \"MN\", county = tc_counties, \n                   year = 2021, cache = True)\n\ntc_tracts.plot()\n\nUsing FIPS code '27' for input 'MN'\n\n\nUsing FIPS code '053' for input 'Hennepin'\nUsing FIPS code '123' for input 'Ramsey'\nUsing FIPS code '139' for input 'Scott'\nUsing FIPS code '019' for input 'Carver'\nUsing FIPS code '037' for input 'Dakota'\nUsing FIPS code '163' for input 'Washington'\nUsing FIPS code '003' for input 'Anoka'\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nWe’ll next need to grab data on median age and merge to the Census tract shapes. I don’t have plans to implement my R package tidycensus in Python; tidycensus is designed specifically for use within R’s tidyverse and Pythonic interfaces to the Census API like cenpy already exist. However, I’ve always admired Hannah Recht’s work on the R censusapi package, which can connect to all Census Bureau API endpoints. pygris includes a get_census() function inspired by censusapi that developers can use to build interfaces to the data they need.\nLet’s use get_census() to get data on median age at the Census tract level for Minnesota, then merge to our Census tracts for additional analysis.\n\nfrom pygris import validate_state\nfrom pygris.data import get_census\n\nmn_median_age = get_census(dataset = \"acs/acs5\",\n                           variables = \"B01002_001E\",\n                           year = 2021,\n                           params = {\n                             \"for\": \"tract:*\",\n                             \"in\": f\"state:{validate_state('MN')}\"},\n                           guess_dtypes = True,\n                           return_geoid = True\n)\n\nmn_median_age.head()\n\nUsing FIPS code '27' for input 'MN'\n\n\n\n\n\n\n\n\n\nB01002_001E\nGEOID\n\n\n\n\n1\n57.8\n27001770100\n\n\n2\n51.5\n27001770200\n\n\n3\n46.8\n27001770300\n\n\n4\n57.7\n27001770401\n\n\n5\n49.1\n27001770402\n\n\n\n\n\n\n\nYou can understand the arguments to get_census() as follows:\n\ndataset is the dataset name on the Census API you are connecting to. Datasets can be found at https://api.census.gov/data.html in the “Dataset Name” column.\nvariables is a string (or list of strings) representing the variable IDs you want for a given dataset. For the 2021 5-year ACS, those variable IDs are found at https://api.census.gov/data/2021/acs/acs5/variables.html.\n\nyear is the year of your data (or end-year for a 5-year ACS sample); the Census API will refer to this as the “vintage” of the data.\nparams is a dict of query parameters to send to the API. Each endpoint will have its own parameters, so you’ll need to spend a little time with the Census API documentation to learn what you can use. In our case, we are requesting data for Census tracts in Minnesota. The built-in validate_state() function can be used here to convert 'MN' to an appropriate FIPS code.\n\nguess_dtypes and return_geoid are convenience parameters that you’ll want to use judiciously. guess_dtypes tries to guess which columns to convert to numeric, and return_geoid tries to find columns to concatenate into a GEOID column that can be used for merging to Census shapes. These arguments won’t be appropriate for every API endpoint.\n\nWith our data in hand, we can do an inner merge and map the result:\n\nimport matplotlib.pyplot as plt\n\ntc_tract_age = tc_tracts.merge(mn_median_age, how = \"inner\", on = \"GEOID\")\n\ntc_tract_age.plot(column = \"B01002_001E\", legend = True)\n\nplt.title(\"Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS\")\n\nText(0.5, 1.0, 'Median age by Census tract\\nMinneapolis-St. Paul, 2017-2021 ACS')\n\n\n\n\n\n\n\nAnalyzing spatial clustering with PySAL\nThe PySAL family of Python packages is central to the work of anyone who needs to analyze spatial data in Python. The esda package makes the calculation of the local Moran’s I statistic remarkably smooth. We will generate a Queen’s case spatial weights object (see my book for more technical details) to represent relationships between Census tracts and their neighbors, then call the Moran_Local() function to calculate the LISA statistics.\n\nfrom libpysal import weights\nimport esda\n\ntc_clean = tc_tract_age.copy().dropna().filter(['GEOID', 'B01002_001E', 'geometry'])\n\nw = weights.contiguity.Queen.from_dataframe(tc_clean)\n\nm = esda.Moran_Local(tc_clean['B01002_001E'], w, seed = 1983)\n\ntype(m)\n\nesda.moran.Moran_Local\n\n\nWe get an object of type Moran_Local which stores our analytical result. The documentation provides a comprehensive overview of the object’s attributes. We’ll grab two here and add them back to our Census tract dataset as new columns: q, which is the LISA quadrant (denoting the cluster type); and p_sim, which gives us a pseudo-p-value based on conditional permutation (see here for further discussion of this concept).\nUsing this information, we can identify “significant” and “non-significant” clusters and generate some more informative labels.\n\nimport numpy as np\n\n# We can extract the LISA quadrant along with the p-value from the lisa object\ntc_clean['quadrant'] = m.q\ntc_clean['p_sim'] = m.p_sim\n# Convert all non-significant quadrants to zero\ntc_clean['quadrant'] = np.where(tc_clean['p_sim'] &gt; 0.05, 0, tc_clean['quadrant'])\n\n# Get more informative descriptions\ntc_clean['quadrant'] = tc_clean['quadrant'].replace(\n  to_replace = {\n    0: \"Not significant\",\n    1: \"High-high\",\n    2: \"Low-high\",\n    3: \"Low-low\",\n    4: \"High-low\"\n  }\n)\n\ntc_clean.head()\n\n\n\n\n\n\n\n\nGEOID\nB01002_001E\ngeometry\nquadrant\np_sim\n\n\n\n\n0\n27053103700\n29.3\nPOLYGON ((-93.25825 44.98358, -93.25790 44.983...\nNot significant\n0.070\n\n\n1\n27053104100\n28.2\nPOLYGON ((-93.31847 44.98174, -93.31847 44.983...\nLow-low\n0.043\n\n\n2\n27053104400\n32.4\nPOLYGON ((-93.28158 44.97790, -93.28153 44.978...\nNot significant\n0.228\n\n\n3\n27053105100\n44.6\nPOLYGON ((-93.32873 44.96012, -93.32873 44.960...\nNot significant\n0.127\n\n\n4\n27053105400\n33.2\nPOLYGON ((-93.26972 44.96807, -93.26926 44.969...\nNot significant\n0.310\n\n\n\n\n\n\n\n\n\nBuilding an interactive LISA map\nWe now have all the information necessary to map LISA clusters. I’m going to show a workflow that differs slightly from typical LISA maps like the one illustrated in my book. One disadvantage of static LISA maps is that they assume an analyst has familiarity with the region under study. Without this familiarity, it can be difficult to determine exactly which locations are represented by different cluster types.\nEnter the .explore() GeoDataFrame method in GeoPandas. .explore() is an interface to Leaflet.js through Folium. Simply calling .explore() on a GeoDataFrame gets you started interactively exploring your spatial data; however, the method itself is a fairly full-featured interactive mapping engine.\nWith a little customization, we can build out an informative interactive map showing our LISA analysis of median age by Census tract in the Twin Cities. Here’s how we do it:\n\nWe choose \"quadrant\" as the column to visualize, and pass a list of colors to cmap to align with the typical color scheme used for LISA mapping (with some small modifications to improve visibility).\nlegend = True adds an informative legend, and a muted grey basemap is selected with tiles.\nThe various _kwds parameters are quite powerful, as this is how you will do more fine-grained customization of your map. We’ll reduce the line weight of our polygons to 0.5, and importantly do some customization of the popup to change the column names to informative aliases. Click on a Census tract to see what you get!\n\n\n# Build a LISA cluster map \ntc_clean.explore(column = \"quadrant\", \n                 cmap = [\"red\", \"hotpink\", \"deepskyblue\", \"blue\", \"lightgrey\"], \n                 legend = True, \n                 tiles = \"CartoDB positron\", \n                 style_kwds = {\"weight\": 0.5}, \n                 legend_kwds = { \"caption\": \"LISA quadrant\"}, \n                 tooltip = False, \n                 popup = True,\n                 popup_kwds = {\n                    \"aliases\": [\"GEOID\", \"Median age\", \"LISA quadrant\", \"Pseudo p-value\"]\n                 })\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nOur analytical result shows that younger areas tend to be found nearer to the Minneapolis / St. Paul urban cores, and older areas cluster in the western, southern, and northeastern suburbs. Spatial outliers are scattered throughout the region, and the map’s interactivity allows us to zoom in and click to understand these outliers in greater detail.\nTry out this workflow for yourself, and follow along here for more of my favorite examples from Analyzing US Census Data translated to Python over the next few months."
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "WALKER DATA offers custom data science workshops tailored to the needs of your organization as well as online workshops available for purchase.\nCheck out these testimonials from workshop participants:\nReach out to kyle@walker-data.com to discuss your workshop needs today, or purchase a recorded workshop from the links below."
  },
  {
    "objectID": "workshops.html#gis-and-interactive-mapping-workshops",
    "href": "workshops.html#gis-and-interactive-mapping-workshops",
    "title": "Workshops",
    "section": "GIS and Interactive Mapping Workshops",
    "text": "GIS and Interactive Mapping Workshops\nGet started with Geographic Infomation Systems, spatial data analysis, and interactive mapping in R in 5 hours of custom workshops. The workshops are available for purchase separately or as a bundle for a $100 discount!\n\n\n\n    Buy the bundle on Gumroad\n\n\n\n\nWorkshop 1: Getting Started With Geographic Information Systems in R\n\n\n\n\n\nCensus data from R shown in ArcGIS Pro\n\n\nIn this 2.5 hour workshop, you will gain the skills you need to get started with Geographic Information Systems using the R programming language. You’ll learn the fundamentals of doing GIS in R, gain experience with R’s powerful toolset for spatial data analysis, and learn how to connect R to desktop GIS tools like ArcGIS and QGIS.\n\n    Buy on Gumroad\n\n\n\n\nWorkshop 2: Interactive Mapping with R\n\n\n\n\n\nA linked interactive map and chart with ggiraph - no Shiny required!\n\n\nIn this workshop, you will learn how to make elegant interactive maps with minimal code using the R programming language. You’ll gain experience with a range of libraries for interactive mapping in R; learn how to customize and share interactive maps; and dynamically link maps and charts using R.\n\n    Buy on Gumroad"
  },
  {
    "objectID": "workshops.html#census-workshops",
    "href": "workshops.html#census-workshops",
    "title": "Workshops",
    "section": "2020 Census workshops",
    "text": "2020 Census workshops\nLearn all about using the new 2020 Decennial Census data with R and tidycensus in 5 hours of custom workshops. You can purchase each workshop separately, or buy the workshop bundle and save $100!\n\n    Buy the bundle on Gumroad\n\n\n\n\nWorkshop 1: Access and Analyze the New 2020 Decennial Census Data\n\n\n\n\n\nPopulation pyramid for Lane County, OR\n\n\nIn this workshop, you’ll learn how to analyze the 2020 Decennial Census’s Demographic and Housing Characteristics file using R and the tidycensus R package. Topics covered will include brand-new functionality in tidycensus to acquire and process data from the 2020 DHC file to help you analyze the data right away. You’ll gain experience with the tidyverse ecosystem of tools to reveal insights in the new data, and you’ll learn how to create stunning data visualizations to communicate those insights.\n\n    Buy on Gumroad\n\n\n\n\nWorkshop 2: Mapping and Spatial Analysis with the 2020 Decennial Census\n\n\n\n\n\nMap of same-sex households in San Francisco\n\n\nMaps are powerful tools for communicating insights in the 2020 DHC data. In this workshop, you’ll learn how to use R and tidycensus to get spatial Census data seamlessly, and visualize the new data with a range of cartographic tools. You’ll also learn all about Census geography and how to perform powerful spatial analyses with 2020 Census data.\n\n    Buy on Gumroad\n\n\n\nYou get the following materials with your workshop purchase:\n\nUnlimited access to the workshop videos\nCustom tutorials on working with Census data not posted anywhere else\n\nFor updates on these workshops and more data analysis content, subscribe to the Walker Data mailing list:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "WALKER DATA is a consultancy helping clients solve a wide range of data science problems. We specialize in the following areas:\n\nSpatial data science and Geographic Information Systems (GIS)\nDemographics and location intelligence\nPredictive analytics and machine learning\nData visualization and dashboard development with tools like Shiny, Mapbox, and Tableau\nTraining and custom workshops to help your organization use R, Python, Census data, and GIS tools more effectively\n\n\n\n\nBook a discovery call today!\n\n\nWALKER DATA is led by its principal, Kyle Walker, Ph.D. Kyle is an internationally-recognized researcher, consultant, and software developer in the field of spatial data science, and was the 2022 recipient of the Spatial Data Scientist of the Year award from the software company CARTO. He is the author of the book Analyzing US Census Data: Methods, Maps, and Models in R, and has published several popular software packages for spatial data science in R and Python that have been collectively downloaded over 1 million times.\nKyle also serves as Director of Research for the Linnaean Company, a data science and strategy firm helping companies with business and location intelligence solutions. Learn more about the Linnaean Company here."
  },
  {
    "objectID": "index.html#gis-demographics-and-data-science-consulting",
    "href": "index.html#gis-demographics-and-data-science-consulting",
    "title": "WALKER DATA",
    "section": "GIS, Demographics, and Data Science Consulting",
    "text": "GIS, Demographics, and Data Science Consulting\nWALKER DATA is a data science consultancy specializing in geospatial and demographic data support for your business.\nPlease reach out to kyle@walker-data.com for consulting assistance with the following topics:\n\nDemographic analysis / Census data\nBusiness and location intelligence\nGeographic Information Systems (GIS) and spatial data science\nCustom data science workshops, including training in the R and Python programming languages\n\n\n\n\n\n\n\n  \n    \n      \n      Expert demographics, Census, and data science advisory\n    \n  \n  \n    \n      \n      Sophisticated location intelligence solutions for your business\n    \n  \n  \n    \n      \n      R and GIS training from the developer of the tidycensus R package\n    \n  \n  \n    \n      \n      Read the book Analyzing US Census Data: Methods, Maps, and Models in R\n    \n  \n\n\n\n\nTo receive on-going updates from Walker Data, consider signing up for the Walker Data mailing list:"
  },
  {
    "objectID": "posts/indexed-mapping/index.html",
    "href": "posts/indexed-mapping/index.html",
    "title": "Iterative indexed ‘mapping’ in R",
    "section": "",
    "text": "My book Analyzing US Census Data: Methods, Maps, and Models in R, published last year, covers a lot of the data science tips and tricks I’ve learned over the years. In my academic and consulting work, I apply a lot of additional workflows that the book doesn’t cover. This year on the blog, I’d like to share some brief examples of workflows I’ve found useful with a focus on applications to Census and demographic data.\nIn my consulting work, I’m commonly asked to build out maps, charts, or reports for a large number of cities or regions at once. The goal here is often to allow for rapid exploration / iteration, so a basic map template might be fine. Doing this for a few cities one-by-one isn’t a problem, but it quickly gets tedious when you have dozens, if not hundreds, of visuals to produce – and keeping all the results organized can be a pain.\nI’m a huge fan of R’s list data structure to help with these tasks. I struggled with lists in R when I was first learning the language, but I now find lists essential. Lists are flexible data structures that can basically store whatever objects you want. I’m partial to the named list, in which those objects are accessible by name."
  },
  {
    "objectID": "posts/indexed-mapping/index.html#getting-demographic-data-with-tidycensus",
    "href": "posts/indexed-mapping/index.html#getting-demographic-data-with-tidycensus",
    "title": "Iterative indexed ‘mapping’ in R",
    "section": "Getting demographic data with tidycensus",
    "text": "Getting demographic data with tidycensus\nLet’s tackle an example. We’re tasked with creating interactive, exploratory maps of the percent of the workforce who works from home by Census tract for the 100 largest metro areas in the United States. First, we’ll need to identify the 100 largest metro areas in the US - a job for the tidycensus R package.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\ntop100metros &lt;- get_acs(\n  geography = \"cbsa\",\n  variables = \"B01003_001\",\n  year = 2022,\n  survey = \"acs1\",\n  geometry = TRUE\n) %&gt;%\n  slice_max(estimate, n = 100)\n\nWe’ve pulled data here from the 2022 1-year American Community Survey for core-based statistical areas (CBSAs), which includes metropolitan statistical areas. slice_max() gets us the 100 largest values of the estimate column, which in this case stores values on total population in 2022.\nNext, we’ll need to grab data on the work-from-home share by Census tract. Census tracts aren’t directly identifiable by metro, so we’ll get data for the entire US. With tidycensus, grabbing demographic data for all 50 US states + DC and Puerto Rico is straightforward; pass an appropriate vector to the state parameter as an argument.\n\nus_wfh_tract &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"DP03_0024P\",\n  state = c(state.abb, \"DC\", \"PR\"),\n  year = 2022,\n  geometry = TRUE\n)"
  },
  {
    "objectID": "posts/indexed-mapping/index.html#iterative-spatial-overlay",
    "href": "posts/indexed-mapping/index.html#iterative-spatial-overlay",
    "title": "Iterative indexed ‘mapping’ in R",
    "section": "Iterative spatial overlay",
    "text": "Iterative spatial overlay\n\nlibrary(sf)\nsf_use_s2(FALSE)\n\nus_wfh_metro &lt;- top100metros %&gt;%\n  split(~NAME) %&gt;%\n  map(~{\n    \n    tract_ids &lt;- us_wfh_tract %&gt;%\n      st_point_on_surface() %&gt;%\n      st_filter(.x) %&gt;%\n      pull(GEOID)\n    \n    us_wfh_tract %&gt;%\n      filter(GEOID %in% tract_ids)\n  })\n\nLet’s examine our data for any metro:\n\nus_wfh_metro$`Baltimore-Columbia-Towson, MD Metro Area`\n\nSimple feature collection with 713 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -77.31136 ymin: 38.71274 xmax: -75.74767 ymax: 39.72131\nGeodetic CRS:  NAD83\n# A tibble: 713 × 6\n   GEOID       NAME            variable estimate   moe                  geometry\n * &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 24013505101 Census Tract 5… DP03_00…     17.8   5.4 (((-76.9689 39.41866, -7…\n 2 24013505208 Census Tract 5… DP03_00…     10.3   5.6 (((-76.99383 39.36777, -…\n 3 24025301602 Census Tract 3… DP03_00…      9.4   4.6 (((-76.30582 39.4331, -7…\n 4 24025306100 Census Tract 3… DP03_00…      4.3   3.4 (((-76.12338 39.53333, -…\n 5 24027606804 Census Tract 6… DP03_00…     20.2   6.2 (((-76.85616 39.17558, -…\n 6 24027606604 Census Tract 6… DP03_00…     12.3   4.7 (((-76.83466 39.20062, -…\n 7 24510151200 Census Tract 1… DP03_00…      8.1   8.9 (((-76.66506 39.33338, -…\n 8 24510270702 Census Tract 2… DP03_00…      6     5.3 (((-76.57356 39.36854, -…\n 9 24510271802 Census Tract 2… DP03_00…      2.6   3.5 (((-76.68352 39.34294, -…\n10 24510272004 Census Tract 2… DP03_00…     20    13.8 (((-76.69473 39.36801, -…\n# ℹ 703 more rows"
  },
  {
    "objectID": "posts/indexed-mapping/index.html#making-maps-with-an-indexed-map",
    "href": "posts/indexed-mapping/index.html#making-maps-with-an-indexed-map",
    "title": "Iterative indexed ‘mapping’ in R",
    "section": "Making maps with an indexed map()",
    "text": "Making maps with an indexed map()\n\nlibrary(mapview)\n\nwfh_maps &lt;- imap(us_wfh_metro, ~{\n  mapview(\n    .x, \n    zcol = \"estimate\",\n    layer.name = glue::glue(\"% working from home&lt;br&gt;{.y}\")\n  )\n})\n\nWhat does the map look like?\n\nwfh_maps$`Dallas-Fort Worth-Arlington, TX Metro Area`"
  },
  {
    "objectID": "posts/iterative-mapping/index.html",
    "href": "posts/iterative-mapping/index.html",
    "title": "Iterative ‘mapping’ in R",
    "section": "",
    "text": "My book Analyzing US Census Data: Methods, Maps, and Models in R, published last year, covers a lot of the data science tips and tricks I’ve learned over the years. In my academic and consulting work, I apply a lot of additional workflows that the book doesn’t cover. This year on the blog, I’d like to share some brief examples of workflows I’ve found useful with a focus on applications to Census and demographic data.\nIn my consulting work, I’m commonly asked to build out maps, charts, or reports for a large number of cities or regions at once. The goal here is often to allow for rapid exploration / iteration, so a basic map template might be fine. Doing this for a few cities one-by-one isn’t a problem, but it quickly gets tedious when you have dozens, if not hundreds, of visuals to produce – and keeping all the results organized can be a pain.\nLet’s tackle a hypothetical example. Your boss has assigned you the following task:\nAt first, this may seem like a fairly significant research task. You need to do the following:\nThis might seem like a lot of work, but it really isn’t too bad if you handle it the right way with R."
  },
  {
    "objectID": "posts/iterative-mapping/index.html#getting-demographic-data-with-tidycensus",
    "href": "posts/iterative-mapping/index.html#getting-demographic-data-with-tidycensus",
    "title": "Iterative ‘mapping’ in R",
    "section": "Getting demographic data with tidycensus",
    "text": "Getting demographic data with tidycensus\nWe’ll first need to identify the 100 largest metro areas in the US; fortunately, this is straightforward to accomplish with the tidycensus R package.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\ntop100metros &lt;- get_acs(\n  geography = \"cbsa\",\n  variables = \"B01003_001\",\n  year = 2022,\n  survey = \"acs1\",\n  geometry = TRUE\n) %&gt;%\n  slice_max(estimate, n = 100)\n\nWe’ve pulled data here from the 2022 1-year American Community Survey for core-based statistical areas (CBSAs), which includes metropolitan statistical areas. slice_max() gets us the 100 largest values of the estimate column, which in this case stores values on total population in 2022.\nNext, we’ll need to grab data on the work-from-home share by Census tract, a reasonably granular Census geography with approximately 4,000 residents on average. Census tracts aren’t directly identifiable by metro, so we’ll get data for the entire US. With tidycensus, grabbing demographic data for all 50 US states + DC and Puerto Rico is straightforward; pass an appropriate vector to the state parameter as an argument.\n\nus_wfh_tract &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"DP03_0024P\",\n  state = c(state.abb, \"DC\", \"PR\"),\n  year = 2022,\n  geometry = TRUE\n)\n\nus_wfh_tract\n\nSimple feature collection with 85396 features and 5 fields (with 337 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: 17.88328 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\n# A tibble: 85,396 × 6\n   GEOID       NAME            variable estimate   moe                  geometry\n   &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 01089003100 Census Tract 3… DP03_00…     16     5.9 (((-86.59838 34.74091, -…\n 2 01089000501 Census Tract 5… DP03_00…      3.2   3.2 (((-86.65703 34.77881, -…\n 3 01089011021 Census Tract 1… DP03_00…     11.2   6.4 (((-86.78678 34.67045, -…\n 4 01095031200 Census Tract 3… DP03_00…      3.6   3.6 (((-86.17402 34.23036, -…\n 5 01073012401 Census Tract 1… DP03_00…      4.9   6.8 (((-86.90739 33.57447, -…\n 6 01073000800 Census Tract 8… DP03_00…      8.4   8.8 (((-86.86691 33.53711, -…\n 7 01073010402 Census Tract 1… DP03_00…      2.7   2.7 (((-86.99094 33.37425, -…\n 8 01073003900 Census Tract 3… DP03_00…      3.1   4.3 (((-86.87669 33.49688, -…\n 9 01073005908 Census Tract 5… DP03_00…      3.8   4.3 (((-86.689 33.60838, -86…\n10 01103000100 Census Tract 1… DP03_00…      0     2.1 (((-86.97709 34.60967, -…\n# ℹ 85,386 more rows\n\n\nAs you can see above, we’ve fetched data on the share of the workforce working from home for all Census tracts in the United States."
  },
  {
    "objectID": "posts/iterative-mapping/index.html#iterative-spatial-overlay",
    "href": "posts/iterative-mapping/index.html#iterative-spatial-overlay",
    "title": "Iterative ‘mapping’ in R",
    "section": "Iterative spatial overlay",
    "text": "Iterative spatial overlay\nOur next step is to determine which of these Census tracts fall within each of the top 100 metro areas in the US. One way to accomplish this is with a spatial join; I’m going to show you an alternative workflow that I’ll call “iterative spatial overlay” which I like to use to organize my data.\nI’m a huge fan of R’s list data structure to help with these tasks. I struggled with lists in R when I was first learning the language, but I now find lists essential. Lists are flexible data structures that can basically store whatever objects you want. I’m partial to the named list, in which those objects are accessible by name.\nThe code below is a simplified version of a workflow I commonly use. The split() command will split the top100metros object into a list of metro areas, organized by the name of the metro. We then iterate over this list with map(), doing a series of spatial analysis operations for each metro. In this case, I’m identifying which tracts fall within each metro area, first performing a spatial filter on tract points then filtering on those tract IDs. This helps circumvent any topology issues with polygon-on-polygon overlay.\nOne note - as I’m using formula notation with map(), each respective object in the list will be represented by .x as map() iterates over my list.\n\nlibrary(sf)\nsf_use_s2(FALSE)\n\ntract_points &lt;- us_wfh_tract %&gt;%\n  st_point_on_surface()\n\nus_wfh_metro &lt;- top100metros %&gt;%\n  split(~NAME) %&gt;%\n  map(~{\n    tract_ids &lt;- tract_points %&gt;%\n      st_filter(.x) %&gt;%\n      pull(GEOID)\n    \n    us_wfh_tract %&gt;%\n      filter(GEOID %in% tract_ids)\n  })\n\nWe now have the results of every operation organized by the metro area’s name. In RStudio, type us_wfh_metro followed by the $ sign, then use the Tab key to browse through the various metro areas.\n\nus_wfh_metro$`Minneapolis-St. Paul-Bloomington, MN-WI Metro Area`\n\nSimple feature collection with 892 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -94.26152 ymin: 44.19584 xmax: -92.13481 ymax: 46.24697\nGeodetic CRS:  NAD83\n# A tibble: 892 × 6\n   GEOID       NAME            variable estimate   moe                  geometry\n * &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 27053021505 Census Tract 2… DP03_00…     11.8   4   (((-93.40064 45.01667, -…\n 2 27053026713 Census Tract 2… DP03_00…     18.2   4.5 (((-93.43159 45.0667, -9…\n 3 27053000102 Census Tract 1… DP03_00…     15.3   6.8 (((-93.29919 45.05114, -…\n 4 27053108700 Census Tract 1… DP03_00…     30.4   8.5 (((-93.24235 44.94837, -…\n 5 27053102100 Census Tract 1… DP03_00…      4.3   2.9 (((-93.3082 45.00603, -9…\n 6 27053026814 Census Tract 2… DP03_00…      4.1   2.4 (((-93.3211 45.10899, -9…\n 7 27053023802 Census Tract 2… DP03_00…     25.4   6.3 (((-93.32898 44.88885, -…\n 8 27053100800 Census Tract 1… DP03_00…     21.2  13.3 (((-93.30833 45.02409, -…\n 9 27053125800 Census Tract 1… DP03_00…      7.5   5.1 (((-93.26259 44.95375, -…\n10 27053025403 Census Tract 2… DP03_00…     12.2   5.4 (((-93.28698 44.83271, -…\n# ℹ 882 more rows"
  },
  {
    "objectID": "posts/iterative-mapping/index.html#using-map-to-make-maps",
    "href": "posts/iterative-mapping/index.html#using-map-to-make-maps",
    "title": "Iterative ‘mapping’ in R",
    "section": "Using map() to make… maps",
    "text": "Using map() to make… maps\nOne of the things I really like about list data structures is that I can use the map() family of functions in the purrr package (or lapply() if you prefer base R) to visualized my data and return those visualizations in the same organized format. Here, I’ll use map() to make 100 maps - one for each metro area.\n\nlibrary(mapview)\nlibrary(leaflet)\n\nwfh_maps &lt;- map(us_wfh_metro, ~{\n  mapview(\n    .x, \n    zcol = \"estimate\",\n    layer.name = \"% working from home\"\n  ) \n})\n\nAs before, I can access each interactive map by name! This is often how I’ll keep my data organized and explore the various maps as needed.\n\nwfh_maps$`Dallas-Fort Worth-Arlington, TX Metro Area`\n\n\n\n\n\n\nFinally, I can also readily write out my maps to static screenshots using a similar workflow. In this case I’ll use an indexed walk() to step over each interactive map and write it to a static file with mapshot(). Note that the iwalk() function gives me access to .x, which is the list element (the map itself), and .y, which is the index - a character string representing the metro area’s name in this instance. I can use .y as the name of the file as well as the title of the map which will be added to the output.\n\nlibrary(glue)\n\niwalk(wfh_maps, ~{\n  out_file &lt;- glue(\"img/{.y}.png\")\n\n  .x@map %&gt;%\n    addControl(.y) %&gt;%\n    mapshot(file = out_file)\n})\n\n\n\n\n\n\n\n\n\n\nI now have 100 static maps in my img folder! They won’t be uniquely customized for each metro, but when time is of the essence, this workflow is often “good enough.”"
  },
  {
    "objectID": "posts/accessibility-surface/index.html",
    "href": "posts/accessibility-surface/index.html",
    "title": "Visualizing accessibility surfaces in R",
    "section": "",
    "text": "In November, I completed the 30 Day Map Challenge for the first time. I posted all of my submissions to Twitter/X and LinkedIn, and observed how the community reacted to each of my maps through likes, reposts, and comments.\nMy most popular submission based on social media engagement was for Day 21: Raster. I’ve been using a technique for years called an “accessibility surface” to visualize proximity to locations. The accessibility surface is a raster dataset in which each grid cell represents the travel-time to that location from another given location, or the nearest location in a set of locations. Accessibility surfaces are useful tools for commute and transportation planning, understanding capacity of emergency services, visualizing access to amenities, and more.\nThe map I submitted showed accessibility from Nike Headquarters in the Portland metropolitan area. Let’s walk through how to create it!\nTo get started, we’ll need to identify our location that we want to calculate access from, then build out a dataset that represents accessibility to that location. I’ll use Nike Headquarters in Beaverton, Oregon for this example. This workflow could be used to plan residential locations for commuters considering jobs at Nike, or current workers thinking about where to relocate.\nI’m a fan of Mapbox’s tools for computing accessibility due to their ease of use, especially through the mapboxapi R package that I wrote. You’ll need a Mapbox account for this to work, and to set your Mapbox access token, which requires a credit card to register. If you’d prefer not to go this route, you might consider building isochrones with self-hosted options like OSRM or Valhalla.\nThe first step I use when computing an accessibility surface is to create layered isochrones. An isochrone is a shape that represents the reachable area from a given location in a particular amount of time for a given travel mode. I’ve written the function mb_isochrone() to make the calculation of isochrones in R straightforward; here, we’ll compute layered isochrones at 1-minute drivetime intervals around the Nike HQ.\n\nlibrary(mapboxapi)\nlibrary(leaflet)\n\nisos &lt;- mb_isochrone(\n  location = \"One Bowerman Dr, Beaverton, OR 97005\",\n  profile = \"driving\",\n  time = 1:45\n)\n\nleaflet() %&gt;%\n  addMapboxTiles(\"streets-v11\", \"mapbox\") %&gt;%\n  addPolygons(data = isos)\n\n\n\n\n\nThe visualization is messy - that’s because we have 45 different isochrones drawn on top of one another. We’ll want to clean this up by converting to a raster using the accessibility surface visualization method. To accomplish this, we’ll turn to the fasterize R package, a package that offers speedy tools for vector-to-raster data conversion.\nWe’ll first transform our data to a projected coordinate reference system and define a raster template with 100m grid cells. The fasterize() function then computes, for each grid cell, the minimum overlapping time as defined by the isochrones that overlap the raster.\n\nlibrary(sf)\nlibrary(fasterize)\n\nisos_proj &lt;- st_transform(isos, 32618)\n\ntemplate &lt;- raster(isos_proj, resolution = 100)\n\niso_surface &lt;- fasterize(isos_proj, template, field = \"time\", fun = \"min\")\n\niso_surface\n\nclass      : RasterLayer \ndimensions : 939, 695, 652605  (nrow, ncol, ncell)\nresolution : 100, 100  (x, y)\nextent     : -3210927, -3141427, 6239990, 6333890  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 45  (min, max)\n\n\nWe note that the raster dataset has a resolution of 100 meters, with values between 1 and 45 minutes for each of its 652,000 grid cells. Let’s take a look at this on the map!\n\npal &lt;- colorNumeric(\"plasma\", isos$time, na.color = \"transparent\")\n\nnike_map &lt;- leaflet() %&gt;%\n  addMapboxTiles(style_id = \"light-v9\",\n                 username = \"mapbox\",\n                 scaling_factor = \"0.5x\") %&gt;%\n  addRasterImage(iso_surface, colors = pal, opacity = 0.5) %&gt;%\n  addLegend(values = isos$time, pal = pal,\n            title = \"Drive-time from&lt;br&gt;Nike HQ\")\n\nnike_map\n\n\n\n\n\nThe surface shows travel-times from Nike’s headquarters in a much smoother way. Far-out areas are visualized in yellow, whereas nearby areas are shown in purple; you can also see the purple “arteries” of the highway system around Portland.\nYou may also want to add a marker to the Leaflet map showing exactly where Nike’s headquarters is located. Here’s an updated map:\n\nnike_icon &lt;- makeIcon(\n  iconUrl = \"https://nike.com/favicon.ico\",\n  iconWidth = 25,\n  iconHeight = 25\n)\n\nnike_hq &lt;- mb_geocode(\"One Bowerman Dr, Beaverton, OR 97005\")\n\nnike_map %&gt;%\n  addMarkers(lng = nike_hq[1], lat = nike_hq[2], icon = nike_icon)\n\n\n\n\n\nTry out the accessibility surface for yourselves, and let me know what you create! oughts"
  }
]